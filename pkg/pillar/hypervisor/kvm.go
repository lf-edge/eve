// Copyright (c) 2017-2024 Zededa, Inc.
// SPDX-License-Identifier: Apache-2.0

package hypervisor

import (
	"context"
	"fmt"
	"io"
	"net"
	"net/http"
	"os"
	"path/filepath"
	"runtime"
	"strings"
	"sync/atomic"
	"text/template"
	"time"

	zconfig "github.com/lf-edge/eve-api/go/config"
	"github.com/lf-edge/eve/pkg/pillar/agentlog"
	"github.com/lf-edge/eve/pkg/pillar/containerd"
	"github.com/lf-edge/eve/pkg/pillar/types"
	"github.com/lf-edge/eve/pkg/pillar/utils"
	fileutils "github.com/lf-edge/eve/pkg/pillar/utils/file"
	uuid "github.com/satori/go.uuid"
	"github.com/sirupsen/logrus"
	"golang.org/x/sys/unix"
)

const (
	// KVMHypervisorName is a name of kvm hypervisor
	KVMHypervisorName  = "kvm"
	minUringKernelTag  = uint64((5 << 16) | (4 << 8) | (72 << 0))
	swtpmTimeout       = 10 // seconds
	qemuTimeout        = 3  // seconds
	vtpmPurgeEndpoint  = "purge"
	vtpmTermEndpoint   = "terminate"
	vtpmLaunchEndpoint = "launch"
)

var (
	clientCid  = uint32(unix.VMADDR_CID_HOST + 1)
	vTPMClient = &http.Client{
		Transport: vtpmClientUDSTransport(),
		Timeout:   5 * time.Second,
	}
)

// vtpmRequestResult holds the result of a vTPM request.
type vtpmRequestResult struct {
	Body  string
	Error error
}

// We build device model around PCIe topology according to best practices
//    https://github.com/qemu/qemu/blob/master/docs/pcie.txt
// and
//    https://libvirt.org/pci-hotplug.html
// Thus the only PCI devices plugged directly into the root (pci.0) bus are:
//    00:01.0 cirrus-vga
//    00:02.0 pcie-root-port for QEMU XHCI Host Controller
//    00:03.0 virtio-serial for hvc consoles and serial communications with the domain
//    00:0x.0 pcie-root-port for block or network device #x (where x > 2)
//    00:0y.0 virtio-9p-pci
//
// This makes everything but 9P volumes be separated from root pci bus
// and effectively hang off the bus of its own:
//     01:00.0 QEMU XHCI Host Controller (behind pcie-root-port 00:02.0)
//     xx:00.0 block or network device #x (behind pcie-root-port 00:0x.0)
//
// It would be nice to figure out how to do the same with virtio-9p-pci
// eventually, but for now this is not a high priority.
//
// As discussed in https://edk2.groups.io/g/discuss/topic/windows_2019_vm_fails_to_boot/74465994
// I/O size exceeds the max SCSI I/O limitation(8M) of vhost-scsi in KVM
// we adjust max_sectors option (16384) to run Windows VM with vhost-scsi-pci and avoid errors like
// [ 259.573575] vhost_scsi_calc_sgls: requested sgl_count: 2649 exceeds pre-allocated max_sgls: 2048

const qemuGlobalConfTemplate = `# This file is automatically generated by domainmgr
[msg]
  timestamp = "on"

[machine]
  type = "{{.Machine}}"
  dump-guest-core = "off"
{{- if eq .Machine "virt" }}
  accel = "kvm:tcg"
  gic-version = "host"
{{- end -}}
{{- if ne .Machine "virt" }}
  accel = "kvm"
  vmport = "off"
  kernel-irqchip = "on"
{{- end -}}
{{- if .DomainConfig.BootLoader }}
  {{- if or (ne .VirtualizationMode "FML") (eq .Machine "virt") }}
  firmware = "{{.DomainConfig.BootLoader}}"
  {{- end }}
{{- end -}}
{{- if .DomainConfig.Kernel }}
  kernel = "{{.DomainConfig.Kernel}}"
{{- end -}}
{{- if .DomainConfig.Ramdisk }}
  initrd = "{{.DomainConfig.Ramdisk}}"
{{- end -}}
{{- if .DomainConfig.DeviceTree }}
  dtb = "{{.DomainConfig.DeviceTree}}"
{{- end -}}
{{- if .DomainConfig.ExtraArgs }}
  append = "{{.DomainConfig.ExtraArgs}}"
{{- end }}
{{if ne .Machine "virt" }}
[global]
  driver = "kvm-pit"
  property = "lost_tick_policy"
  value = "delay"

[global]
  driver = "ICH9-LPC"
  property = "disable_s3"
  value = "1"

[global]
  driver = "ICH9-LPC"
  property = "disable_s4"
  value = "1"

[rtc]
  base = "localtime"
  driftfix = "slew"

[device]
  driver = "intel-iommu"
  caching-mode = "on"
{{- end }}

{{- if and (eq .VirtualizationMode "FML") (ne .Machine "virt") }}

[drive "drive-ovmf-code"]
  if = "pflash"
  format = "raw"
  readonly = "on"
  unit = "0"
  file = "{{.DomainConfig.BootLoader}}"

[drive "drive-ovmf-vars"]
  if = "pflash"
  format = "raw"
  unit = "1"
  file = "{{.BootLoaderSettingsFile}}"
{{- end }}

[overcommit]
  mem-lock = "off"

[chardev "charmonitor"]
  backend = "socket"
  path = "` + kvmStateDir + `{{.DomainConfig.DisplayName}}/qmp"
  server = "on"
  wait = "off"

[mon "monitor"]
  chardev = "charmonitor"
  mode = "control"

[chardev "charlistener"]
  backend = "socket"
  path = "` + kvmStateDir + `{{.DomainConfig.DisplayName}}/listener.qmp"
  server = "on"
  wait = "off"

[mon "listener"]
  chardev = "charlistener"
  mode = "control"

[memory]
  size = "{{.DomainConfig.Memory}}"

[smp-opts]
  cpus = "{{.DomainConfig.VCpus}}"
  sockets = "1"
  cores = "{{.DomainConfig.VCpus}}"
  threads = "1"

[device]
  driver = "virtio-serial"
  addr = "3"

[chardev "charserial0"]
  backend = "socket"
  mux = "on"
  path = "` + kvmStateDir + `{{.DomainConfig.DisplayName}}/cons"
  server = "on"
  wait = "off"
  logfile = "/dev/fd/1"
  logappend = "on"

[device]
  driver = "virtconsole"
  chardev = "charserial0"
  name = "org.lfedge.eve.console.0"

{{if .DomainConfig.IsOCIContainer -}}
[chardev "charserial1"]
  backend = "socket"
  path = "` + kvmStateDir + `{{.DomainConfig.DisplayName}}/shim-cons"
  server = "on"
  wait = "off"

{{if .DomainConfig.EnableVncShimVM -}}
[chardev "charserial2"]
  backend = "vc"
{{- end}}

[chardev "charhub0"]
  backend = "hub"
  chardevs.0 = "charserial1"
{{- if .DomainConfig.EnableVncShimVM}}
  chardevs.1 = "charserial2"
{{- end}}
  logfile = "/dev/fd/1"
  logappend = "on"

[device]
  driver = "virtconsole"
  chardev = "charhub0"
  name = "org.lfedge.eve.console.shim"
{{end}}
{{if .DomainConfig.EnableVnc}}
[vnc "default"]
  vnc = "0.0.0.0:{{if .DomainConfig.VncDisplay}}{{.DomainConfig.VncDisplay}}{{else}}0{{end}}"
  to = "99"
{{- if .DomainConfig.VncPasswd}}
  password = "on"
{{- end -}}
{{end}}
#[device "video0"]
#  driver = "qxl-vga"
#  ram_size = "67108864"
#  vram_size = "67108864"
#  vram64_size_mb = "0"
#  vgamem_mb = "16"
#  max_outputs = "1"
#  bus = "pcie.0"
#  addr = "0x1"
{{ if ne .DomainConfig.GPUConfig "" -}}
{{- if ne .Machine "virt" }}
[device "video0"]
  driver = "VGA"
  vgamem_mb = "16"
  bus = "pcie.0"
  addr = "0x1"
{{else}}
[device "video0"]
  driver = "virtio-gpu-pci"
{{end}}
{{- end}}
[device "pci.2"]
  driver = "pcie-root-port"
  port = "12"
  chassis = "2"
  bus = "pcie.0"
  addr = "0x2"

[device "usb"]
  driver = "qemu-xhci"
  p2 = "15"
  p3 = "15"
  bus = "pci.2"
  addr = "0x0"
{{if ne .Machine "virt" }}
[device "input0"]
  driver = "usb-tablet"
  bus = "usb.0"
  port = "1"
{{else}}
[device "input0"]
  driver = "usb-kbd"
  bus = "usb.0"
  port = "1"

[device "input1"]
  driver = "usb-mouse"
  bus = "usb.0"
  port = "2"
{{end}}`

const qemuDiskTemplate = `
{{if eq .Devtype "cdrom"}}
[drive "drive-sata0-{{.DiskID}}"]
  file = "{{.FileLocation}}"
  format = "{{.Format | Fmt}}"
  if = "none"
  media = "cdrom"
  readonly = "on"

[device "sata0-{{.SATAId}}"]
  drive = "drive-sata0-{{.DiskID}}"
{{- if eq .Machine "virt"}}
  driver = "usb-storage"
{{else}}
  driver = "ide-cd"
  bus = "ide.{{.SATAId}}"
{{- end }}
{{else if eq .Devtype "9P"}}
[fsdev "fsdev{{.DiskID}}"]
  fsdriver = "local"
  security_model = "none"
  multidevs = "remap"
  path = "{{.FileLocation}}"

[device "fs{{.DiskID}}"]
  driver = "virtio-9p-pci"
  fsdev = "fsdev{{.DiskID}}"
  mount_tag = "share_dir"
  addr = "{{printf "0x%x" .PCIId}}"
{{else}}
[device "pci.{{.PCIId}}"]
  driver = "pcie-root-port"
  port = "1{{.PCIId}}"
  chassis = "{{.PCIId}}"
  bus = "pcie.0"
  addr = "{{printf "0x%x" .PCIId}}"
{{if eq .WWN ""}}
[drive "drive-virtio-disk{{.DiskID}}"]
  file = "{{.FileLocation}}"
  format = "{{.Format | Fmt}}"
  aio = "{{.AioType}}"
  cache = "writeback"
  if = "none"
{{if .ReadOnly}}  readonly = "on"{{end}}
{{- if eq .Devtype "legacy"}}
[device "ahci.{{.PCIId}}"]
  bus = "pci.{{.PCIId}}"
  driver = "ahci"

[device "ahci-disk{{.DiskID}}"]
  driver = "ide-hd"
  bus = "ahci.{{.PCIId}}.0"
{{- else}}
[device "virtio-disk{{.DiskID}}"]
  driver = "virtio-blk-pci"
  scsi = "off"
  bus = "pci.{{.PCIId}}"
  addr = "0x0"
{{- end}}
  drive = "drive-virtio-disk{{.DiskID}}"
{{- else}}
[device "vhost-disk{{.DiskID}}"]
  driver = "vhost-scsi-pci"
  max_sectors = "16384"
  wwpn = "{{.WWN}}"
  bus = "pci.{{.PCIId}}"
  addr = "0x0"
  num_queues = "{{.NumQueues}}"
{{- end}}
{{end}}`

const qemuNetTemplate = `
[device "pci.{{.PCIId}}"]
  driver = "pcie-root-port"
  port = "1{{.PCIId}}"
  chassis = "{{.PCIId}}"
  bus = "pcie.0"
  multifunction = "on"
  addr = "{{printf "0x%x" .PCIId}}"

[netdev "hostnet{{.NetID}}"]
  type = "tap"
  ifname = "{{.Vif}}"
  br = "{{.Bridge}}"
  script = "/etc/xen/scripts/qemu-ifup"
  downscript = "no"
{{- if eq .Driver "virtio-net-pci" }}
  vhost = "on"
{{- end}}

[device "net{{.NetID}}"]
  driver = "{{.Driver}}"
  netdev = "hostnet{{.NetID}}"
  mac = "{{.Mac}}"
  bus = "pci.{{.PCIId}}"
  addr = "0x0"
{{- if and (eq .Driver "virtio-net-pci") (ne .MTU 0) }}
  host_mtu = "{{.MTU}}"
{{- end}}
`

const qemuPCIeRootPortTemplate = `
[device "pci.{{.PCIId}}"]
  driver = "pcie-root-port"
  port = "1{{.PCIId}}"
  chassis = "{{.PCIId}}"
  bus = "pcie.0"
  multifunction = "on"
  addr = "{{printf "0x%x" .PCIId}}"
`

const qemuPCIeBridgeTemplate = `
[device "pcie-bridge.{{.PCIId}}"]
  driver = "pcie-pci-bridge"
  bus = "pci.{{.PCIId}}"
  addr = "0x0"
`

const qemuPCIPassthruTemplate = `
[device]
  driver = "vfio-pci"
  host = "{{.PciShortAddr}}"
  bus = "{{.Bus}}"
  addr = "{{.Addr}}"
{{- if .Xvga }}
  x-vga = "on"
{{- end -}}
{{- if .Xopregion }}
  x-igd-opregion = "on"
{{- end}}
`

const qemuSerialTemplate = `
[chardev "charserial-usr{{.ID}}"]
  backend = "serial"
  path = "{{.SerialPortName}}"

[device "serial-usr{{.ID}}"]
{{- if eq .Machine "virt"}}
  driver = "pci-serial"
{{- else}}
  driver = "isa-serial"
{{- end}}
  chardev = "charserial-usr{{.ID}}"
`

const qemuCANBusTemplate = `
[object "canbus{{.ID}}"]
  qom-type = "can-bus"

[device "{{.IfName}}"]
  driver = "kvaser_pci"
  canbus = "canbus{{.ID}}"

[object "canhost{{.ID}}"]
  qom-type = "can-host-socketcan"
  canbus = "canbus{{.ID}}"
  if = "{{.HostIfName}}"
`

const qemuVsockTemplate = `
[device "eve-vsock0"]
  driver = "vhost-vsock-pci"
  disable-legacy = "on"
  guest-cid = "{{.GuestCID}}"
`

const qemuSwtpmTemplate = `
[chardev "swtpm"]
  backend = "socket"
  path = "{{.CtrlSocket}}"

[tpmdev "tpm0"]
  type = "emulator"
  chardev = "swtpm"

[device "tpm-tis"]
# 'virt' refers to aarch64
# 'tpm-tis-device' for aarch64 versus 'tpm-tis' for x86
# Reference: https://listman.redhat.com/archives/libvir-list/2021-February/msg00647.html
{{- if eq .Machine "virt"}}
  driver = "tpm-tis-device"
{{- else}}
  driver = "tpm-tis"
{{- end}}
  tpmdev = "tpm0"
`

// Context for qemuGlobalConfTemplate.
type tQemuGlobalConfContext struct {
	Machine                string
	VirtualizationMode     string
	BootLoaderSettingsFile string
	types.DomainConfig
	types.DomainStatus
}

// Context for qemuSwtpmTemplate.
type tQemuSwtpmContext struct {
	Machine    string
	CtrlSocket string
}

// Context for qemuPCIPassthruTemplate.
type tQemuPCIPassthruContext struct {
	PciShortAddr string
	Xvga         bool
	Xopregion    bool
	Bus          string
	Addr         string
}

// Context for qemuPCIeRootPortTemplate.
type tQemuPCIeRootPortContext struct {
	PCIId int
}

// Context for qemuPCIeBridgeTemplate.
type tQemuPCIeBridgeContext struct {
	PCIId int
}

// Context for qemuNetTemplate.
type tQemuNetContext struct {
	PCIId, NetID     int
	Driver           string
	Mac, Bridge, Vif string
	MTU              uint16
}

// Context for qemuSerialTemplate.
type tQemuSerialContext struct {
	Machine        string
	SerialPortName string
	ID             int
}

// Context for qemuCANBusTemplate.
type tQemuCANBusContext struct {
	Machine    string
	IfName     string
	HostIfName string
	ID         int
}

// Context for qemuDiskTemplate.
type tQemuDiskContext struct {
	Machine                          string
	PCIId, DiskID, SATAId, NumQueues int
	AioType                          string
	types.DiskStatus
}

// Context for qemuVsockTemplate.
type tQemuVsockContext struct {
	GuestCID string
}

var (
	tQemuGlobalConf, tQemuSwtmp, tQemuVsock              *template.Template
	tQemuPCIeBridge, tQemuPCIPassthru, tQemuPCIeRootPort *template.Template
	tQemuDisk, tQemuNet, tQemuSerial, tQemuCANBus        *template.Template
)

// Initialize all Go templates used to generate qemu config file.
func init() {
	var err error
	tQemuGlobalConf, err = template.New("qemuGlobalConf").Parse(qemuGlobalConfTemplate)
	if err != nil {
		panic(fmt.Errorf("parsing qemuGlobalConfTemplate failed: %w", err))
	}
	tQemuSwtmp, err = template.New("qemuSwtpm").Parse(qemuSwtpmTemplate)
	if err != nil {
		panic(fmt.Errorf("parsing qemuSwtpmTemplate failed: %w", err))
	}
	tQemuVsock, err = template.New("qemuVsock").Parse(qemuVsockTemplate)
	if err != nil {
		panic(fmt.Errorf("parsing qemuVsockTemplate failed: %w", err))
	}
	tQemuPCIeBridge, err = template.New("qemuPCIeBridge").Parse(qemuPCIeBridgeTemplate)
	if err != nil {
		panic(fmt.Errorf("parsing qemuPCIeBridgeTemplate failed: %w", err))
	}
	tQemuPCIPassthru, err = template.New("qemuPCIPassthru").Parse(qemuPCIPassthruTemplate)
	if err != nil {
		panic(fmt.Errorf("parsing qemuPCIPassthruTemplate failed: %w", err))
	}
	tQemuPCIeRootPort, err = template.New("qemuPCIeRootPort").Parse(qemuPCIeRootPortTemplate)
	if err != nil {
		panic(fmt.Errorf("parsing qemuPCIeRootPortTemplate failed: %w", err))
	}
	tQemuDisk, err = template.New("qemuDisk").
		Funcs(template.FuncMap{"Fmt": func(f zconfig.Format) string {
			return strings.ToLower(f.String())
		}}).
		Parse(qemuDiskTemplate)
	if err != nil {
		panic(fmt.Errorf("parsing qemuDiskTemplate failed: %w", err))
	}
	tQemuNet, err = template.New("qemuNet").Parse(qemuNetTemplate)
	if err != nil {
		panic(fmt.Errorf("parsing qemuNetTemplate failed: %w", err))
	}
	tQemuSerial, err = template.New("qemuSerial").Parse(qemuSerialTemplate)
	if err != nil {
		panic(fmt.Errorf("parsing qemuSerialTemplate failed: %w", err))
	}
	tQemuCANBus, err = template.New("qemuCANBus").Parse(qemuCANBusTemplate)
	if err != nil {
		panic(fmt.Errorf("parsing qemuCANBusTemplate failed: %w", err))
	}
}

const kvmStateDir = "/run/hypervisor/kvm/"
const sysfsPciDriversProbe = "/sys/bus/pci/drivers_probe"
const vfioDriverPath = "/sys/bus/pci/drivers/vfio-pci"

// KvmContext is a KVM domains map 0-1 to anchor device model UNIX processes (qemu or firecracker)
// For every anchor process we maintain the following entry points in the
// /run/hypervisor/kvm/DOMAIN_NAME:
//
//	 pid - contains PID of the anchor process
//	 qmp - UNIX domain socket that allows us to talk to anchor process
//	cons - symlink to /dev/pts/X that allows us to talk to the serial console of the domain
//
// In addition to that, we also maintain DOMAIN_NAME -> PID mapping in KvmContext, so we don't
// have to look things up in the filesystem all the time (this also allows us to filter domains
// that may be created by others)
type KvmContext struct {
	ctrdContext
	// for now the following is statically configured and can not be changed per domain
	devicemodel  string
	dmExec       string
	dmArgs       []string
	dmCPUArgs    []string
	dmFmlCPUArgs []string
	capabilities *types.Capabilities
}

func newKvm() Hypervisor {
	ctrdCtx, err := initContainerd()
	if err != nil {
		logrus.Fatalf("couldn't initialize containerd (this should not happen): %v. Exiting.", err)
		return nil // it really never returns on account of above
	}
	// later on we may want to pass device model machine type in DomainConfig directly;
	// for now -- lets just pick a static device model based on the host architecture
	// "-cpu host",
	// -cpu IvyBridge-IBRS,ss=on,vmx=on,movbe=on,hypervisor=on,arat=on,tsc_adjust=on,mpx=on,rdseed=on,smap=on,clflushopt=on,sha-ni=on,umip=on,md-clear=on,arch-capabilities=on,xsaveopt=on,xsavec=on,xgetbv1=on,xsaves=on,pdpe1gb=on,3dnowprefetch=on,avx=off,f16c=off,hv_time,hv_relaxed,hv_vapic,hv_spinlocks=0x1fff
	switch runtime.GOARCH {
	case "arm64":
		return KvmContext{
			ctrdContext:  *ctrdCtx,
			devicemodel:  "virt",
			dmExec:       "/usr/lib/xen/bin/qemu-system-aarch64",
			dmArgs:       []string{"-display", "none", "-S", "-no-user-config", "-nodefaults", "-no-shutdown", "-serial", "chardev:charserial0"},
			dmCPUArgs:    []string{"-cpu", "host"},
			dmFmlCPUArgs: []string{"-cpu", "host"},
		}
	case "amd64":
		return KvmContext{
			ctrdContext:  *ctrdCtx,
			devicemodel:  "pc-q35-3.1",
			dmExec:       "/usr/lib/xen/bin/qemu-system-x86_64",
			dmArgs:       []string{"-display", "none", "-S", "-no-user-config", "-nodefaults", "-no-shutdown", "-serial", "chardev:charserial0", "-machine", "hpet=off"},
			dmCPUArgs:    []string{"-cpu", "host"},
			dmFmlCPUArgs: []string{"-cpu", "host,hv_time,hv_relaxed,hv_vendor_id=eveitis,hypervisor=off,kvm=off"},
		}
	}
	return nil
}

// GetCapabilities returns capabilities of the kvm hypervisor
func (ctx KvmContext) GetCapabilities() (*types.Capabilities, error) {
	if ctx.capabilities != nil {
		return ctx.capabilities, nil
	}
	vtd, err := ctx.checkIOVirtualisation()
	if err != nil {
		return nil, fmt.Errorf("fail in check IOVirtualization: %v", err)
	}
	ctx.capabilities = &types.Capabilities{
		HWAssistedVirtualization: true,
		IOVirtualization:         vtd,
		CPUPinning:               true,
		UseVHost:                 true,
	}
	return ctx.capabilities, nil
}

// CountMemOverhead - returns the memory overhead estimation for a domain.
func (ctx KvmContext) CountMemOverhead(domainName string, domainUUID uuid.UUID, domainRAMSize int64, vmmMaxMem int64,
	domainMaxCpus int64, domainVCpus int64, domainIoAdapterList []types.IoAdapter, aa *types.AssignableAdapters,
	globalConfig *types.ConfigItemValueMap) (uint64, error) {
	result, err := vmmOverhead(domainName, domainUUID, domainRAMSize, vmmMaxMem, domainMaxCpus, domainVCpus, domainIoAdapterList, aa, globalConfig)
	return uint64(result), err
}

func (ctx KvmContext) checkIOVirtualisation() (bool, error) {
	f, err := os.Open("/sys/kernel/iommu_groups")
	if err == nil {
		files, err := f.Readdirnames(0)
		if err != nil {
			return false, err
		}
		if len(files) != 0 {
			return true, nil
		}
	}
	return false, err
}

// Name returns the name of the kvm hypervisor
func (ctx KvmContext) Name() string {
	return KVMHypervisorName
}

// Task returns either the kvm context or the containerd context depending on the domain status
func (ctx KvmContext) Task(status *types.DomainStatus) types.Task {
	if status.VirtualizationMode == types.NOHYPER {
		return ctx.ctrdContext
	}
	return ctx
}

func estimatedVMMOverhead(domainName string, aa *types.AssignableAdapters, domainAdapterList []types.IoAdapter,
	domainUUID uuid.UUID, domainRAMSize int64, domainMaxCpus int64, domainVcpus int64) (int64, error) {
	var overhead int64

	mmioOverhead, err := mmioVMMOverhead(domainName, aa, domainAdapterList, domainUUID)

	if err != nil {
		return 0, logError("mmioVMMOverhead() failed for domain %s: %v",
			domainName, err)
	}
	overhead = undefinedVMMOverhead() + ramVMMOverhead(domainRAMSize) +
		qemuVMMOverhead() + cpuVMMOverhead(domainMaxCpus, domainVcpus) + mmioOverhead

	return overhead, nil
}

func ramVMMOverhead(ramMemory int64) int64 {
	// 0.224% of the total RAM allocated for VM in bytes
	// this formula is precise and well explained in the following QEMU issue:
	// https://gitlab.com/qemu-project/qemu/-/issues/1003
	// This is a best case scenario because it assumes that all PTEs are allocated
	// sequentially. In reality, there will be some fragmentation and the overhead
	// for now 2.5% (~10x) is a good approximation until we have a better way to
	// predict the memory usage of the VM.
	return ramMemory * 1024 * 25 / 1000
}

// overhead for qemu binaries and libraries
func qemuVMMOverhead() int64 {
	return 20 << 20 // Mb in bytes
}

// overhead for VMM memory mapped IO
// it fluctuates between 0.66 and 0.81 % of MMIO total size
// for all mapped devices. Set it to 1% to be on the safe side
// this can be a pretty big number for GPUs with very big
// aperture size (e.g. 64G for NVIDIA A40)
func mmioVMMOverhead(domainName string, aa *types.AssignableAdapters, domainAdapterList []types.IoAdapter,
	domainUUID uuid.UUID) (int64, error) {
	var pciAssignments []pciDevice
	var mmioSize uint64

	for _, adapter := range domainAdapterList {
		logrus.Debugf("processing adapter %d %s\n", adapter.Type, adapter.Name)
		aaList := aa.LookupIoBundleAny(adapter.Name)
		// We reserved it in handleCreate so nobody could have stolen it
		if len(aaList) == 0 {
			return 0, logError("IoBundle disappeared %d %s for %s\n",
				adapter.Type, adapter.Name, domainName)
		}
		for _, ib := range aaList {
			if ib == nil {
				continue
			}
			if ib.UsedByUUID != domainUUID {
				return 0, logError("IoBundle not ours %s: %d %s for %s\n",
					ib.UsedByUUID, adapter.Type, adapter.Name,
					domainName)
			}
			if ib.PciLong != "" && ib.UsbAddr == "" {
				logrus.Infof("Adding PCI device <%s>\n", ib.PciLong)
				tap := pciDevice{pciLong: ib.PciLong, ioType: ib.Type}
				pciAssignments = addNoDuplicatePCI(pciAssignments, tap)
			}
		}
	}

	for _, dev := range pciAssignments {
		logrus.Infof("PCI device %s %d\n", dev.pciLong, dev.ioType)
		// read the size of the PCI device aperture. Only GPU/VGA devices for now
		if dev.ioType != types.IoOther && dev.ioType != types.IoHDMI {
			continue
		}
		// skip bridges
		isBridge, err := dev.isBridge()
		if err != nil {
			// do not treat as fatal error
			logrus.Warnf("Can't read PCI device class, treat as bridge %s: %v\n",
				dev.pciLong, err)
			isBridge = true
		}

		if isBridge {
			logrus.Infof("Skipping bridge %s\n", dev.pciLong)
			continue
		}

		// read all resources of the PCI device
		resources, err := dev.readResources(sysfsPciDevices)
		if err != nil {
			return 0, logError("Can't read PCI device resources %s: %v\n",
				dev.pciLong, err)
		}

		// calculate the size of the MMIO region
		for _, res := range resources {
			if res.valid() && res.isMem() {
				mmioSize += res.size()
			}
		}
	}

	// 1% of the total MMIO size in bytes
	mmioOverhead := int64(mmioSize) / 100

	logrus.Infof("MMIO size: %d / overhead: %d for %s", mmioSize, mmioOverhead, domainName)

	return int64(mmioOverhead), nil
}

// each vCPU requires about 3MB of memory
func cpuVMMOverhead(maxCpus int64, vcpus int64) int64 {
	cpus := maxCpus
	if cpus == 0 {
		cpus = vcpus
	}
	return cpus * (3 << 20) // Mb in bytes
}

// memory allocated by QEMU for its own purposes.
// statistical analysis did not revile any correlation between
// VM configuration (devices, nr of vcpus, etc) and this number
// however the size of disk space affects it. Probably some internal
// QEMU caches are allocated based on the size of the disk image.
// it requires more investigation.
func undefinedVMMOverhead() int64 {
	return 350 << 20 // Mb in bytes
}

func vmmOverhead(domainName string, domainUUID uuid.UUID, domainRAMSize int64, vmmMaxMem int64, domainMaxCpus int64, domainVCpus int64, domainIoAdapterList []types.IoAdapter, aa *types.AssignableAdapters, globalConfig *types.ConfigItemValueMap) (int64, error) {
	var overhead int64

	// Fetch VMM max memory setting (aka vmm overhead)
	overhead = vmmMaxMem << 10

	// Global node setting has a higher priority
	if globalConfig != nil {
		VmmOverheadOverrideCfgItem, ok := globalConfig.GlobalSettings[types.VmmMemoryLimitInMiB]
		if !ok {
			return 0, logError("Missing key %s", string(types.VmmMemoryLimitInMiB))
		}
		if VmmOverheadOverrideCfgItem.IntValue > 0 {
			overhead = int64(VmmOverheadOverrideCfgItem.IntValue) << 20
		}
	}

	if overhead == 0 {
		overhead, err := estimatedVMMOverhead(domainName, aa, domainIoAdapterList, domainUUID, domainRAMSize, domainMaxCpus, domainVCpus)
		if err != nil {
			return 0, logError("estimatedVMMOverhead() failed for domain %s: %v",
				domainName, err)
		}
		return overhead, nil
	}

	return overhead, nil
}

func getOVMFSettingsFilename(domainName string) (string, error) {
	// Extract the UUID from the domain name. It lets persist the OVMF settings file over domain
	// configuration changes, including domain reactivation (each time the configuration is changed,
	// it updates the domain name, increasing the counter after UUID).
	domainUUID, _, _, err := types.DomainnameToUUID(domainName)
	if err != nil {
		return "", logError("failed to extract UUID from domain name: %v", err)
	}
	return types.OVMFSettingsDir + "/" + domainUUID.String() + "_OVMF_VARS.fd", nil
}

func prepareOVMFSettings(config types.DomainConfig, status types.DomainStatus, globalConfig *types.ConfigItemValueMap) error {
	// Create the OVMF settings directory if it does not exist
	if err := os.MkdirAll(types.OVMFSettingsDir, 0755); err != nil {
		return logError("failed to create OVMF settings directory: %v", err)
	}
	// Create a copy of the ovmf_vars.bin file in <domainName>_ovmf_vars.bin
	ovmfSettingsFile, err := getOVMFSettingsFilename(status.DomainName)
	if err != nil {
		return logError("failed to get OVMF settings file: %v", err)
	}
	// Check if we need custom OVMF settings for the domain (the resolution)
	fmlResolution := types.FmlResolutionUnset
	if config.VirtualizationMode == types.FML {
		// if we are not getting the resolution from the cloud-init, check the
		// global config.
		fmlResolution = status.FmlCustomResolution
		if fmlResolution == types.FmlResolutionUnset {
			if fmlResolution, err = getFmlCustomResolution(&status, globalConfig); err != nil {
				return logError("failed to get custom resolution for domain %s: %v", status.DomainName, err)
			}
		}
	}
	// Find the necessary OVMF settings file
	ovmfSettingsFileSrc := types.OVMFSettingsTemplate
	if fmlResolution != types.FmlResolutionUnset {
		ovmfSettingsFileSrc = types.CustomOVMFSettingsDir + "/OVMF_VARS_" + fmlResolution + ".fd"
	}
	if _, err := os.Stat(ovmfSettingsFile); os.IsNotExist(err) {
		if err := fileutils.CopyFile(ovmfSettingsFileSrc, ovmfSettingsFile); err != nil {
			return logError("failed to copy OVMF_VARS file: %v", err)
		}
	}
	// Set the RW permissions for the OVMF settings file
	if err := os.Chmod(ovmfSettingsFile, 0666); err != nil {
		return logError("failed to set RW permissions for ovmf_vars.bin file: %v", err)
	}
	return nil
}

func cleanupOVMFSettings(domainName string) error {
	ovmfVarsFile, err := getOVMFSettingsFilename(domainName)
	if err != nil {
		return logError("failed to get OVMF settings file: %v", err)
	}
	if err := os.Remove(ovmfVarsFile); err != nil {
		return logError("failed to remove ovmf_vars.bin file: %v", err)
	}
	return nil
}

// Setup sets up kvm
func (ctx KvmContext) Setup(status types.DomainStatus, config types.DomainConfig,
	aa *types.AssignableAdapters, globalConfig *types.ConfigItemValueMap, file *os.File) error {

	diskStatusList := status.DiskStatusList
	domainName := status.DomainName
	domainUUID := status.UUIDandVersion.UUID

	// check if vTPM is enabled
	swtpmCtrlSock := ""
	if status.VirtualTPM {
		domainUUID, _, _, err := types.DomainnameToUUID(domainName)
		if err != nil {
			logError("failed to extract UUID from domain name (vTPM): %v", err)
		} else {
			swtpmCtrlSock = fmt.Sprintf(types.SwtpmCtrlSocketPath, domainUUID)
		}
	}

	// Before we start building the domain config, we need to prepare the OVMF settings.
	// Currently, we only support OVMF settings for FML mode on x86_64 architecture.
	// To support OVMF settings for ARM, we need to add fix OVFM build for ARM to
	// produce separate OVMF_VARS.fd and OVMF_CODE.fd files. Currently, OVMF build
	// for ARM produces a single QEMU_EFI.fd file that contains both OVMF_VARS.fd
	// and OVMF_CODE.fd.
	if config.VirtualizationMode == types.FML && runtime.GOARCH == "amd64" {
		if err := prepareOVMFSettings(config, status, globalConfig); err != nil {
			return logError("failed to setup OVMF settings for domain %s: %v", status.DomainName, err)
		}
	}

	// first lets build the domain config
	if err := ctx.CreateDomConfig(domainName, config, status, diskStatusList,
		aa, globalConfig, swtpmCtrlSock, file); err != nil {
		return logError("failed to build domain config: %v", err)
	}

	dmArgs := ctx.dmArgs
	if config.VirtualizationMode == types.FML {
		dmArgs = append(dmArgs, ctx.dmFmlCPUArgs...)
	} else {
		dmArgs = append(dmArgs, ctx.dmCPUArgs...)
	}

	if config.MetaDataType == types.MetaDataOpenStack {
		// we need to set product_name to support cloud-init
		dmArgs = append(dmArgs, "-smbios", "type=1,product=OpenStack Compute")
	}

	os.MkdirAll(kvmStateDir+domainName, 0777)

	args := []string{ctx.dmExec}
	args = append(args, dmArgs...)
	args = append(args, "-name", domainName,
		"-uuid", domainUUID.String(),
		"-readconfig", file.Name(),
		"-pidfile", kvmStateDir+domainName+"/pid")

	// Add CPUs affinity as a parameter to qemu.
	// It's not supported to be configured in the .ini file so we need to add it here.
	// The arguments are in the format of: -object thread-context,id=tc1,cpu-affinity=0-1,cpu-affinity=6-7
	// The thread-context object is introduced in qemu 7.2
	if config.CPUsPinned {
		// Create the thread-context object string
		threadContext := "thread-context,id=tc1"
		for _, cpu := range status.CPUs {
			// Add the cpu-affinity arguments to the thread-context object
			threadContext += fmt.Sprintf(",cpu-affinity=%d", cpu)
		}
		args = append(args, "-object", threadContext)
	}

	spec, err := ctx.setupSpec(&status, &config, status.OCIConfigDir)

	if err != nil {
		return logError("failed to load OCI spec for domain %s: %v", status.DomainName, err)
	}
	if err = spec.AddLoader(xenToolsPath); err != nil {
		return logError("failed to add kvm hypervisor loader to domain %s: %v", status.DomainName, err)
	}
	overhead, err := vmmOverhead(domainName, domainUUID, int64(config.Memory), int64(config.VMMMaxMem), int64(config.MaxCpus), int64(config.VCpus), config.IoAdapterList, aa, globalConfig)
	if err != nil {
		return logError("vmmOverhead() failed for domain %s: %v",
			status.DomainName, err)
	}
	logrus.Debugf("Qemu overhead for domain %s is %d bytes", status.DomainName, overhead)
	spec.AdjustMemLimit(config, overhead)
	spec.Get().Process.Args = args
	logrus.Infof("Hypervisor args: %v", args)

	spec.GrantFullAccessToDevices()

	if err := spec.CreateContainer(true); err != nil {
		return logError("Failed to create container for task %s from %v: %v", status.DomainName, config, err)
	}

	return nil
}

// Coalesce per-app `EnableVncShimVM` flag and global `debug.enable.vnc.shim.vm`
// debug flag, making sure we don't activate VNC for shim VM if VNC for
// this application is disabled.
func isVncShimVMEnabled(
	globalConfig *types.ConfigItemValueMap, config types.DomainConfig) bool {
	globalShimVnc := false
	if globalConfig != nil {
		item, ok := globalConfig.GlobalSettings[types.VncShimVMAccess]
		globalShimVnc = ok && item.BoolValue
	}
	return config.EnableVnc && (config.EnableVncShimVM || globalShimVnc)
}

func getFmlCustomResolution(status *types.DomainStatus, globalConfig *types.ConfigItemValueMap) (string, error) {
	fmlResolutions := status.FmlCustomResolution
	// if not set in the domain status, try to get it from the global config
	if fmlResolutions == types.FmlResolutionUnset {
		if globalConfig != nil {
			item, ok := globalConfig.GlobalSettings[types.FmlCustomResolution]
			if ok {
				fmlResolutions = item.StringValue()
			}
		}
	}

	// validate the resolution
	switch fmlResolutions {
	case types.FmlResolution800x600,
		types.FmlResolution1024x768,
		types.FmlResolution1280x800,
		types.FmlResolution1920x1080,
		types.FmlResolutionUnset:
		return fmlResolutions, nil
	}

	return "", fmt.Errorf("invalid fml resolution %s", fmlResolutions)
}

type virtualNetwork struct {
	types.VifConfig
	networkID   int
	pciDeviceID int
}

type pciAddressAllocator struct {
	pciAssignments           []pciDevice
	virtualNetworks          []virtualNetwork
	multifunctionDevices     multifunctionDevs
	firstFreePCIID           int
	enforceNetInterfaceOrder bool
}

// allocate sets pciDeviceID and pciBridgeID for every pciDevice and virtualNetwork
// based on user-configured ordering requirements.
func (a *pciAddressAllocator) allocate() error {
	if !a.enforceNetInterfaceOrder {
		// Fallback to legacy ordering of PCI devices.
		return a.allocateLegacy()
	}

	// Determine PCI addresses for virtual network interfaces, which are connected
	// to the root bus using root ports.
	for i := range a.virtualNetworks {
		pciDeviceID := a.firstFreePCIID
		// Increment pciDeviceID by 1 for every (virtual or assigned) PCI device
		// which should have lower PCI address.
		// For a multifunction PCI device, we increment by 1 for the entire group of
		// functions, as they share a single address on the root bus through their bridge.
		for j := range a.virtualNetworks {
			if i == j {
				continue
			}
			if a.virtualNetworks[j].VifOrder < a.virtualNetworks[i].VifOrder {
				pciDeviceID++
			}
		}
		for pciAddr, md := range a.multifunctionDevices {
			isBefore, isAfter := md.compareOrderWithVirtNet(a.virtualNetworks[i])
			invalidOrder := isBefore && isAfter
			if invalidOrder {
				return logError("Invalid VIF %s configuration: user-defined network "+
					"interface order disrupts the function sequence of the multifunction "+
					"PCI device %s. Interleaving PCI device functions with other devices "+
					"is not allowed.", a.virtualNetworks[i].Vif, pciAddr)
			}
			if isBefore {
				pciDeviceID++
			}
		}
		a.virtualNetworks[i].pciDeviceID = pciDeviceID
	}

	// Determine PCI addresses for direct PCI assignments.
	for i := range a.pciAssignments {
		pciLongWoFunc, err := a.pciAssignments[i].pciLongWOFunction()
		if err != nil {
			logrus.Warnf("retrieving pci address without function failed: %v", err)
			continue
		}
		md := a.multifunctionDevices[pciLongWoFunc]
		if md == nil {
			// Even when device is not multifunction, it still should have entry
			// in the multifunctionDevices map.
			logrus.Warnf("missing multifunctionDevices entry for pci address: %s",
				pciLongWoFunc)
			continue
		}
		// Increment pciDeviceOrBridgeID by 1 for every (virtual or assigned) PCI device
		// which should have lower PCI address.
		// For a multifunction PCI device, we increment by 1 for the entire group of
		// functions, as they share a single address on the root bus through their bridge.
		pciDeviceOrBridgeID := a.firstFreePCIID
		for _, virtNet := range a.virtualNetworks {
			// Order validity already checked when addresses for virtual networks
			// were determined.
			if isBefore, _ := md.compareOrderWithVirtNet(virtNet); !isBefore {
				// Also increased when order is undefined, i.e. this PCI device
				// does not have network function. Non-networking PCI devices
				// are placed after virtual network interfaces.
				pciDeviceOrBridgeID++
			}
		}
		thisHasNetFunc := md.hasNetworkFunction()
		for pciAddr2, md2 := range a.multifunctionDevices {
			if pciLongWoFunc == pciAddr2 {
				continue
			}
			theOtherHasNetFunc := md2.hasNetworkFunction()
			if !thisHasNetFunc {
				if theOtherHasNetFunc {
					// Network functions take priority in the order.
					pciDeviceOrBridgeID++
				} else {
					// Between non-networking devices, order by PCI addresses
					// lexicographically.
					if pciLongWoFunc > pciAddr2 {
						pciDeviceOrBridgeID++
					}
				}
				continue
			}
			if !theOtherHasNetFunc {
				// The other non-network device is ordered after this network device.
				continue
			}
			// Both devices have at least one network function.
			theOtherIsBefore, theOtherIsAfter := md2.compareOrder(*md)
			invalidOrder := theOtherIsBefore && theOtherIsAfter
			if invalidOrder {
				return logError("User-defined network interface order disrupts "+
					"the function sequence of the multifunction PCI devices %s and %s. "+
					"Interleaving PCI device functions with other devices/functions "+
					"is not allowed.", pciLongWoFunc, pciAddr2)
			}
			if theOtherIsBefore {
				pciDeviceOrBridgeID++
			}
		}
		if len(md.devs) > 1 {
			// pciDeviceOrBridgeID is for the bridge wrt. the root bus.
			a.pciAssignments[i].pciBridgeID = pciDeviceOrBridgeID
			// Determine device address on the secondary bus provided by the bridge.
			// Skip PCI address 0 which is unsupported for standard hotplug controller.
			pciDeviceID := 1
			devIndex := md.index(a.pciAssignments[i])
			thisIsNetDev := a.pciAssignments[i].ioType.IsNet()
			for dev2Index, dev2 := range md.devs {
				theOtherIsNetDev := dev2.ioType.IsNet()
				if !thisIsNetDev {
					if theOtherIsNetDev {
						// Network functions take priority in the order.
						pciDeviceID++
					} else {
						// Between non-networking functions, preserve the order received
						// from zedagent.
						if devIndex > dev2Index {
							pciDeviceID++
						}
					}
					continue
				}
				if !theOtherIsNetDev {
					// The other non-network device is ordered after this network device.
					continue
				}
				// Both devices are of the networking type.
				if dev2.netIntfOrder < a.pciAssignments[i].netIntfOrder {
					pciDeviceID++
				}
			}
			a.pciAssignments[i].pciDeviceID = pciDeviceID
		} else {
			// Not multifunction PCI device.
			a.pciAssignments[i].pciBridgeID = 0
			a.pciAssignments[i].pciDeviceID = pciDeviceOrBridgeID
		}
	}
	return nil
}

func (a *pciAddressAllocator) allocateLegacy() error {
	// Virtual network interfaces precede PCI-passthrough devices in the PCI topology.
	// Among virtual interfaces, the order received from zedagent is preserved.
	pciDeviceID := a.firstFreePCIID
	for i := range a.virtualNetworks {
		a.virtualNetworks[i].pciDeviceID = pciDeviceID
		pciDeviceID++
	}

	// Preserve order of PCI assignments as received from zedagent, but group
	// functions of the same multifunction PCI device under the same bridge.
	pciBridgeIDs := make(map[string]int) // key = PCI address without function suffix
	for i := range a.pciAssignments {
		pciLongWoFunc, err := a.pciAssignments[i].pciLongWOFunction()
		if err != nil {
			logrus.Warnf("retrieving pci address without function failed: %v", err)
			continue
		}
		md := a.multifunctionDevices[pciLongWoFunc]
		if md == nil {
			// Even when device is not multifunction, it still should have entry
			// in the a.multifunctionDevices map.
			logrus.Warnf("missing multifunctionDevices entry for pci address: %s",
				pciLongWoFunc)
			continue
		}
		if len(md.devs) > 1 {
			// Multi-function PCI device.
			pciBridgeID, bridgeIDAllocated := pciBridgeIDs[pciLongWoFunc]
			if !bridgeIDAllocated {
				pciBridgeID = pciDeviceID
				pciBridgeIDs[pciLongWoFunc] = pciBridgeID
				pciDeviceID++
			}
			a.pciAssignments[i].pciBridgeID = pciBridgeID
			// Skip PCI address 0 which is unsupported for standard hotplug controller.
			a.pciAssignments[i].pciDeviceID = md.index(a.pciAssignments[i]) + 1
		} else {
			// Not multifunction PCI device.
			a.pciAssignments[i].pciBridgeID = 0
			a.pciAssignments[i].pciDeviceID = pciDeviceID
			pciDeviceID++
		}
	}
	return nil
}

type pciDevicesWithBridge struct {
	bridgeBus string
	devs      []*pciDevice
}

func (pd pciDevicesWithBridge) index(p pciDevice) int {
	for i, dev := range pd.devs {
		if p.sameDevice(*dev) {
			return i
		}
	}
	return -1
}

// compareOrder determines if this (possibly multifunction) device should be ordered
// (in the PCI hierarchy) before the other given device.
// Please note that if both booleans are true, then the user-defined order is invalid
// and would break the (multifunction) device if applied.
// If both return values are false, the order is undefined. This occurs when one or both
// devices lack a network function (user only specifies order for network interfaces).
func (pd pciDevicesWithBridge) compareOrder(
	pd2 pciDevicesWithBridge) (isBefore, isAfter bool) {
	for _, dev := range pd.devs {
		if !dev.ioType.IsNet() {
			continue
		}
		for _, dev2 := range pd2.devs {
			if !dev2.ioType.IsNet() {
				continue
			}
			if dev.netIntfOrder < dev2.netIntfOrder {
				isBefore = true
			}
			if dev.netIntfOrder > dev2.netIntfOrder {
				isAfter = true
			}
		}
	}
	return isBefore, isAfter
}

// compareOrderWithVirtNet determines if this (possibly multifunction) device should be
// ordered (in the PCI hierarchy) before or after the given virtual network device.
// Please note that if both booleans are true, then the user-defined order is invalid
// and would break the (multifunction) device if applied.
// If both return values are false, the order is undefined. This occurs when this device
// lacks a network function (user only specifies order for network interfaces).
func (pd pciDevicesWithBridge) compareOrderWithVirtNet(
	virtNet virtualNetwork) (isBefore, isAfter bool) {
	for _, dev := range pd.devs {
		if !dev.ioType.IsNet() {
			continue
		}
		if dev.netIntfOrder < virtNet.VifOrder {
			isBefore = true
		}
		if dev.netIntfOrder > virtNet.VifOrder {
			isAfter = true
		}
	}
	return isBefore, isAfter
}

// Return true if device has at least one network function.
func (pd pciDevicesWithBridge) hasNetworkFunction() bool {
	for _, dev := range pd.devs {
		if dev.ioType.IsNet() {
			return true
		}
	}
	return false
}

type multifunctionDevs map[string]*pciDevicesWithBridge // key: pci long without function number

func (md multifunctionDevs) isMultiFunction(p pciDevice) bool {
	pciLongWOFunc, err := p.pciLongWOFunction()
	if err != nil {
		return false
	}
	devs, found := md[pciLongWOFunc]
	return found && len(devs.devs) > 1
}

func multifunctionDevGroup(pcis []pciDevice) multifunctionDevs {
	mds := make(multifunctionDevs)

	for i, pa := range pcis {
		pciWithoutFunction, err := pa.pciLongWOFunction()
		if err != nil {
			logrus.Warnf("retrieving pci address without function failed: %v", err)
			continue
		}

		_, found := mds[pciWithoutFunction]
		if !found {
			mds[pciWithoutFunction] = &pciDevicesWithBridge{
				bridgeBus: "",
				devs:      []*pciDevice{},
			}
		}
		mds[pciWithoutFunction].devs = append(mds[pciWithoutFunction].devs, &pcis[i])
	}

	return mds
}

type pciAssignmentsTemplateFiller struct {
	multifunctionDevices multifunctionDevs
	file                 io.Writer
}

func (f *pciAssignmentsTemplateFiller) pciEBridge(pciID int, pciWOFunction string) error {
	pcieBridgeContext := tQemuPCIeBridgeContext{PCIId: pciID}
	if err := tQemuPCIeBridge.Execute(f.file, pcieBridgeContext); err != nil {
		return fmt.Errorf("can't write PCIe bridge Passthrough to config file (%w)", err)
	}
	f.multifunctionDevices[pciWOFunction].bridgeBus = fmt.Sprintf("pcie-bridge.%d", pciID)
	return nil
}

func (f *pciAssignmentsTemplateFiller) do(pciAssignments []pciDevice) error {
	if len(pciAssignments) == 0 {
		return nil
	}

	pciEBridgeForMultiFuncDevCreated := make(map[string]struct{}) // key: pci long without function number
	for _, pa := range pciAssignments {
		pciPTContext := tQemuPCIPassthruContext{
			PciShortAddr: types.PCILongToShort(pa.pciLong),
			Xvga:         pa.isVGA(),
			Xopregion:    false,
		}
		if vendor, err := pa.vid(); err == nil {
			// check for Intel vendor
			if vendor == "0x8086" {
				if pciPTContext.Xvga {
					// we set opregion for Intel vga
					// https://github.com/qemu/qemu/blob/stable-5.0/docs/igd-assign.txt#L91-L96
					pciPTContext.Xopregion = true
				}
			}
		}

		pciLongWoFunc, err := pa.pciLongWOFunction()
		if err != nil {
			logrus.Warnf("retrieving pci address without function failed: %v", err)
			continue
		}
		_, pciEBridgeCreated := pciEBridgeForMultiFuncDevCreated[pciLongWoFunc]
		pciEBridgeForMultiFuncDevCreated[pciLongWoFunc] = struct{}{}
		isMultifunctionDev := f.multifunctionDevices.isMultiFunction(pa)

		// Connect device using PCI Express Root Port either directly or via bridge if it
		// has multiple functions.
		if !isMultifunctionDev || !pciEBridgeCreated {
			var pcieRPContext tQemuPCIeRootPortContext
			if isMultifunctionDev {
				pcieRPContext.PCIId = pa.pciBridgeID
			} else {
				pcieRPContext.PCIId = pa.pciDeviceID
			}
			if err := tQemuPCIeRootPort.Execute(f.file, pcieRPContext); err != nil {
				return logError("can't write PCIe Root Port to config file (%v)", err)
			}
		}
		if isMultifunctionDev {
			if !pciEBridgeCreated {
				err := f.pciEBridge(pa.pciBridgeID, pciLongWoFunc)
				if err != nil {
					logrus.Warnf("could not write template: %v, skipping", err)
				}
			}
			pciPTContext.Bus = f.multifunctionDevices[pciLongWoFunc].bridgeBus
			pciPTContext.Addr = fmt.Sprintf("0x%x", pa.pciDeviceID)
		} else {
			pciPTContext.Bus = fmt.Sprintf("pci.%d", pa.pciDeviceID)
			pciPTContext.Addr = "0x0"
		}
		if err := tQemuPCIPassthru.Execute(f.file, pciPTContext); err != nil {
			return logError("can't write PCI Passthrough to config file (%v)", err)
		}
	}

	return nil
}

type virtNetworkTemplateFiller struct {
	file io.Writer
}

func (f *virtNetworkTemplateFiller) do(virtualNetworks []virtualNetwork,
	virtMode types.VmMode) error {
	for _, virtNet := range virtualNetworks {
		netContext := tQemuNetContext{
			PCIId:  virtNet.pciDeviceID,
			NetID:  virtNet.networkID,
			Mac:    virtNet.Mac.String(),
			Bridge: virtNet.Bridge,
			Vif:    virtNet.Vif,
			MTU:    virtNet.MTU,
		}
		if virtMode == types.LEGACY {
			netContext.Driver = "e1000"
		} else {
			netContext.Driver = "virtio-net-pci"
		}
		if err := tQemuNet.Execute(f.file, netContext); err != nil {
			return logError("failed to write network template to config file: %v", err)
		}
	}
	return nil
}

// CreateDomConfig creates a domain config (a qemu config file,
// typically named something like xen-%d.cfg)
func (ctx KvmContext) CreateDomConfig(domainName string,
	config types.DomainConfig, status types.DomainStatus,
	diskStatusList []types.DiskStatus, aa *types.AssignableAdapters,
	globalConfig *types.ConfigItemValueMap, swtpmCtrlSock string, file *os.File) error {

	virtualizationMode := ""
	bootLoaderSettingsFile, err := getOVMFSettingsFilename(domainName)
	if err != nil {
		return logError("failed to get OVMF settings file: %v", err)
	}
	if config.VirtualizationMode == types.FML {
		virtualizationMode = "FML"
	}
	qemuConfContext := tQemuGlobalConfContext{
		Machine:                ctx.devicemodel,
		VirtualizationMode:     virtualizationMode,
		BootLoaderSettingsFile: bootLoaderSettingsFile,
		DomainConfig:           config,
		DomainStatus:           status,
	}
	qemuConfContext.DomainConfig.Memory = (config.Memory + 1023) / 1024
	qemuConfContext.DomainConfig.EnableVncShimVM =
		isVncShimVMEnabled(globalConfig, config)
	qemuConfContext.DomainConfig.DisplayName = domainName

	// render global device model settings
	if err := tQemuGlobalConf.Execute(file, qemuConfContext); err != nil {
		return logError("can't write to config file %s (%v)", file.Name(), err)
	}

	// render swtpm settings
	if swtpmCtrlSock != "" {
		swtpmContext := tQemuSwtpmContext{
			Machine:    ctx.devicemodel,
			CtrlSocket: swtpmCtrlSock,
		}
		if err := tQemuSwtmp.Execute(file, swtpmContext); err != nil {
			return logError("can't write to config file %s (%v)", file.Name(), err)
		}
	}

	// render disk device model settings
	diskContext := tQemuDiskContext{
		Machine:   ctx.devicemodel,
		PCIId:     4,
		AioType:   "io_uring",
		NumQueues: config.VCpus,
	}
	for _, ds := range diskStatusList {
		if ds.Devtype == "" {
			continue
		}
		if ds.Devtype == "AppCustom" {
			// This is application custom data. It is forwarded to the VM
			// differently - as a download url in zedrouter
			continue
		}
		diskContext.DiskStatus = ds
		if err := tQemuDisk.Execute(file, diskContext); err != nil {
			return logError("can't write to config file %s (%v)", file.Name(), err)
		}
		if diskContext.Devtype == "cdrom" {
			diskContext.SATAId = diskContext.SATAId + 1
		} else {
			diskContext.PCIId = diskContext.PCIId + 1
		}
		diskContext.DiskID = diskContext.DiskID + 1
	}

	var pciAssignments []pciDevice
	var serialAssignments []string
	canBusAssignments := make(map[string]string)
	for _, adapter := range config.IoAdapterList {
		logrus.Debugf("processing adapter %d %s\n", adapter.Type, adapter.Name)
		list := aa.LookupIoBundleAny(adapter.Name)
		// We reserved it in handleCreate so nobody could have stolen it
		if len(list) == 0 {
			logrus.Fatalf("IoBundle disappeared %d %s for %s\n",
				adapter.Type, adapter.Name, domainName)
		}
		for _, ib := range list {
			if ib == nil {
				continue
			}
			if ib.UsedByUUID != config.UUIDandVersion.UUID {
				logrus.Fatalf("IoBundle not ours %s: %d %s for %s\n",
					ib.UsedByUUID, adapter.Type, adapter.Name,
					domainName)
			}
			if ib.PciLong != "" && ib.UsbAddr == "" {
				logrus.Infof("Adding PCI device <%v>\n", ib.PciLong)
				tap := pciDevice{pciLong: ib.PciLong, ioType: ib.Type}
				if ib.Type.IsNet() {
					tap.netIntfOrder = adapter.IntfOrder
				}
				pciAssignments = addNoDuplicatePCI(pciAssignments, tap)
			}
			if ib.Serial != "" {
				logrus.Infof("Adding serial <%s>\n", ib.Serial)
				serialAssignments = addNoDuplicate(serialAssignments, ib.Serial)
			}
			if ib.Type == types.IoLCAN && ib.Ifname != "" {
				var canIfName string
				if ib.Logicallabel == "" {
					canIfName = ib.Phylabel
				} else {
					canIfName = ib.Logicallabel
				}
				if canIfName != "" {
					logrus.Infof("Adding CAN interface <%s>", canIfName)
					if canBusAssignments[canIfName] == "" {
						canBusAssignments[canIfName] = ib.Ifname
					}
				}
			}
		}
	}

	// Group functions of the same multifunction PCI device together.
	multifunctionDevices := multifunctionDevGroup(pciAssignments)

	// Prepare a list of virtual interfaces connecting the application with network
	// instances.
	var virtualNetworks []virtualNetwork
	var networkID int
	for _, vif := range config.VifList {
		virtualNetworks = append(virtualNetworks, virtualNetwork{
			VifConfig: vif,
			networkID: networkID,
		})
		networkID++
	}

	// Determine PCI addresses for all virtual networks and direct PCI assignments.
	addrAllocator := pciAddressAllocator{
		pciAssignments:           pciAssignments,
		virtualNetworks:          virtualNetworks,
		multifunctionDevices:     multifunctionDevices,
		firstFreePCIID:           diskContext.PCIId,
		enforceNetInterfaceOrder: config.EnforceNetworkInterfaceOrder,
	}
	// Set pciDeviceID and pciBridgeID for every item in pciAssignments and virtualNetworks.
	if err = addrAllocator.allocate(); err != nil {
		return logError(err.Error())
	}

	// Render virtual network interfaces.
	virtNetworksFiller := virtNetworkTemplateFiller{
		file: file,
	}
	err = virtNetworksFiller.do(virtualNetworks, config.VirtualizationMode)
	if err != nil {
		return logError(err.Error())
	}

	// Render PCI assignments.
	pciAssignmentsFiller := pciAssignmentsTemplateFiller{
		multifunctionDevices: multifunctionDevices,
		file:                 file,
	}
	err = pciAssignmentsFiller.do(pciAssignments)
	if err != nil {
		return fmt.Errorf("writing to template file %s failed: %w", file.Name(), err)
	}

	// Render serial assignments.
	if len(serialAssignments) != 0 {
		serialPortContext := tQemuSerialContext{Machine: ctx.devicemodel}
		for id, serial := range serialAssignments {
			serialPortContext.SerialPortName = serial
			serialPortContext.ID = id
			if err := tQemuSerial.Execute(file, serialPortContext); err != nil {
				return logError("can't write serial assignment to config file %s (%v)",
					file.Name(), err)
			}
		}
	}

	// Render CANBus assignments.
	if len(canBusAssignments) != 0 {
		canIfContext := tQemuCANBusContext{Machine: ctx.devicemodel}
		id := 0
		for canIf, canHostIf := range canBusAssignments {
			logrus.Infof("CAN interface %s connected to host CAN %s\n", canIf, canHostIf)
			canIfContext.IfName = canIf
			canIfContext.HostIfName = canHostIf
			canIfContext.ID = id
			id++
			if err := tQemuCANBus.Execute(file, canIfContext); err != nil {
				return logError("can't write CAN Bus assignment to config file %s (%v)",
					file.Name(), err)
			}
		}
	}

	// render vsock settings, this should go last to avoid
	// PCI ID conflicts, let qemu assign PCI ID for vsock.
	vsockContext := tQemuVsockContext{
		// currently we don't save the clientCid/AppUUID pair since
		// we there is no need for it, but in the future when wen
		// we add channels for vms to report things like CPU/Mem usage
		// then it makes sense to keep track of who is who.
		GuestCID: fmt.Sprintf("%d",
			// clientCid needs atomic add to avoid race condition
			// in case CreateDomConfig is called concurrently, which
			// happens at least in unit tests.
			atomic.AddUint32(&clientCid, 1)),
	}
	if err := tQemuVsock.Execute(file, vsockContext); err != nil {
		return logError("can't write to config file %s (%v)", file.Name(), err)
	}

	return nil
}

// waitForQmp does the waiting/retry in getQemuStatus, and ignores its returned
// status.
func waitForQmp(domainName string, available bool) error {
	var err error
	sock := GetQmpExecutorSocket(domainName)
	logrus.Infof("waitForQmp for %s %t",
		domainName, available)
	if _, err = getQemuStatus(sock); available == (err == nil) {
		logrus.Infof("waitForQmp for %s %t done", domainName, available)
		return nil
	}
	if available {
		return logError("Giving up waiting to connect to QEMU Monitor Protocol socket %s from VM %s, error: %v",
			sock, domainName, err)
	}
	return logError("Giving up waiting to cleanup VM %s, QEMU Monitor Protocol socket %s is still available",
		domainName, sock)

}

// Start starts a domain
func (ctx KvmContext) Start(domainName string) error {
	logrus.Infof("starting KVM domain %s", domainName)
	if err := ctx.ctrdContext.Start(domainName); err != nil {
		logrus.Errorf("couldn't start task for domain %s: %v", domainName, err)
		return err
	}
	logrus.Infof("done launching qemu device model")
	if err := waitForQmp(domainName, true); err != nil {
		logrus.Errorf("Error waiting for Qmp for domain %s: %v", domainName, err)
		return err
	}
	logrus.Infof("done launching qemu device model")

	qmpFile := GetQmpExecutorSocket(domainName)

	logrus.Debugf("starting qmpEventHandler")
	logrus.Infof("Creating %s at %s", "qmpEventHandler", agentlog.GetMyStack())
	go qmpEventHandler(getQmpListenerSocket(domainName), GetQmpExecutorSocket(domainName))

	annotations, err := ctx.ctrdContext.Annotations(domainName)
	if err != nil {
		logrus.Warnf("Error in get annotations for domain %s: %v", domainName, err)
		return err
	}

	if vncPassword, ok := annotations[containerd.EVEOCIVNCPasswordLabel]; ok && vncPassword != "" {
		if err := execVNCPassword(qmpFile, vncPassword); err != nil {
			return logError("failed to set VNC password %v", err)
		}
	}

	if err := execContinue(qmpFile); err != nil {
		return logError("failed to start domain that is stopped %v", err)
	}

	if status, err := getQemuStatus(qmpFile); err != nil || status != types.RUNNING {
		return logError("domain status is not running but %s after cont command returned %v", status, err)
	}
	return nil
}

// Stop stops a domain
func (ctx KvmContext) Stop(domainName string, _ bool) error {
	if err := execShutdown(GetQmpExecutorSocket(domainName)); err != nil {
		return logError("Stop: failed to execute shutdown command %v", err)
	}
	return nil
}

// Delete deletes a domain
func (ctx KvmContext) Delete(domainName string) (result error) {
	//Sending a stop signal to then domain before quitting. This is done to freeze the domain before quitting it.
	_, err := os.Stat(GetQmpExecutorSocket(domainName))
	if err == nil {
		execStop(GetQmpExecutorSocket(domainName))
		if err = execQuit(GetQmpExecutorSocket(domainName)); err != nil {
			return logError("failed to execute quit command %v", err)
		}
	}
	// we may want to wait a little bit here and actually kill qemu process if it gets wedged
	if err := os.RemoveAll(kvmStateDir + domainName); err != nil {
		return logError("failed to clean up domain state directory %s (%v)", domainName, err)
	}

	return nil
}

// Info returns information of a domain
func (ctx KvmContext) Info(domainName string) (int, types.SwState, error) {
	// first we ask for the task status
	effectiveDomainID, effectiveDomainState, err := ctx.ctrdContext.Info(domainName)
	if err != nil || effectiveDomainState != types.RUNNING {
		return effectiveDomainID, effectiveDomainState, err
	}

	_, err = getQemuStatus(GetQmpExecutorSocket(domainName))
	if err != nil {
		return effectiveDomainID, types.BROKEN,
			logError("couldn't retrieve status for domain %s: %v", domainName, err)
	}

	return effectiveDomainID, effectiveDomainState, nil
}

// Cleanup cleans up a domain
func (ctx KvmContext) Cleanup(domainName string) error {
	if err := ctx.ctrdContext.Cleanup(domainName); err != nil {
		return fmt.Errorf("couldn't cleanup task %s: %v", domainName, err)
	}
	if err := waitForQmp(domainName, false); err != nil {
		return fmt.Errorf("error waiting for Qmp absent for domain %s: %v", domainName, err)
	}

	// Cleanup OVMF settings
	settingsFile, err := getOVMFSettingsFilename(domainName)
	if err != nil {
		return fmt.Errorf("failed to get OVMF settings file: %v", err)
	}
	if _, err := os.Stat(settingsFile); err == nil {
		if err := cleanupOVMFSettings(domainName); err != nil {
			return fmt.Errorf("failed to cleanup OVMF settings for domain %s: %v", domainName, err)
		}
	}
	return nil
}

// PCIReserve reserves a PCI device
func (ctx KvmContext) PCIReserve(long string) error {
	return PCIReserveGeneric(long)
}

// PCIRelease releases the PCI device reservation
func (ctx KvmContext) PCIRelease(long string) error {
	return PCIReleaseGeneric(long)
}

// PCISameController checks if two PCI controllers are the same
func (ctx KvmContext) PCISameController(id1 string, id2 string) bool {
	return PCISameControllerGeneric(id1, id2)
}

func usbBusPort(USBAddr string) (string, string) {
	ids := strings.SplitN(USBAddr, ":", 2)
	if len(ids) == 2 {
		return ids[0], ids[1]
	}
	return "", ""
}

// GetQmpExecutorSocket returns the path to the qmp socket of a domain
func GetQmpExecutorSocket(domainName string) string {
	return filepath.Join(kvmStateDir, domainName, "qmp")
}

func getQmpListenerSocket(domainName string) string {
	return filepath.Join(kvmStateDir, domainName, "listener.qmp")
}

// VirtualTPMSetup launches a vTPM instance for the domain
func (ctx KvmContext) VirtualTPMSetup(domainName string, wp *types.WatchdogParam) error {
	if wp == nil {
		return fmt.Errorf("invalid watchdog configuration")
	}

	domainUUID, _, _, err := types.DomainnameToUUID(domainName)
	if err != nil {
		return fmt.Errorf("failed to extract UUID from domain name: %v", err)
	}
	return requestvTPMLaunch(domainUUID, wp, swtpmTimeout)

}

// VirtualTPMTerminate terminates the vTPM instance
func (ctx KvmContext) VirtualTPMTerminate(domainName string, wp *types.WatchdogParam) error {
	if wp == nil {
		return fmt.Errorf("invalid watchdog configuration")
	}

	domainUUID, _, _, err := types.DomainnameToUUID(domainName)
	if err != nil {
		return fmt.Errorf("failed to extract UUID from domain name: %v", err)
	}
	if err := requestvTPMTermination(domainUUID, wp); err != nil {
		return fmt.Errorf("failed to terminate vTPM for domain %s: %w", domainName, err)
	}
	return nil
}

// VirtualTPMTeardown purges the vTPM instance.
func (ctx KvmContext) VirtualTPMTeardown(domainName string, wp *types.WatchdogParam) error {
	if wp == nil {
		return fmt.Errorf("invalid watchdog configuration")
	}

	domainUUID, _, _, err := types.DomainnameToUUID(domainName)
	if err != nil {
		return fmt.Errorf("failed to extract UUID from domain name: %v", err)
	}
	if err := requestvTPMPurge(domainUUID, wp); err != nil {
		return fmt.Errorf("failed to purge vTPM for domain %s: %w", domainName, err)
	}

	return nil
}

// This is the Unix Domain Socket (UDS) transport for vTPM requests.
func vtpmClientUDSTransport() *http.Transport {
	return &http.Transport{
		DialContext: func(_ context.Context, _, _ string) (net.Conn, error) {
			return net.Dial("unix", types.VtpmdCtrlSocket)
		},
	}
}

func makeRequestAsync(client *http.Client, endpoint, id string, rChan chan<- vtpmRequestResult) {
	url := fmt.Sprintf("http://unix/%s?id=%s", endpoint, id)
	req, err := http.NewRequest(http.MethodGet, url, nil)
	if err != nil {
		rChan <- vtpmRequestResult{Error: fmt.Errorf("error when creating request to %s endpoint: %v", url, err)}
		return
	}
	resp, err := client.Do(req)
	if err != nil {
		rChan <- vtpmRequestResult{Error: fmt.Errorf("error when sending request to %s endpoint: %v", url, err)}
		return
	}
	defer resp.Body.Close()

	body, err := io.ReadAll(resp.Body)
	if err != nil {
		rChan <- vtpmRequestResult{Error: fmt.Errorf("error when reading response body from %s endpoint: %v", url, err)}
		return
	}
	if resp.StatusCode != http.StatusOK {
		rChan <- vtpmRequestResult{Error: fmt.Errorf("received status code %d from %s endpoint", resp.StatusCode, url), Body: string(body)}
		return
	}

	rChan <- vtpmRequestResult{Body: string(body)}
}

func makeRequest(client *http.Client, wk *utils.WatchdogKicker, endpoint, id string) (body string, err error) {
	rChan := make(chan vtpmRequestResult)
	go makeRequestAsync(client, endpoint, id, rChan)

	startTime := time.Now()
	for {
		select {
		case res := <-rChan:
			return res.Body, res.Error
		default:
			utils.KickWatchdog(wk)
			if time.Since(startTime).Seconds() >= float64(client.Timeout.Seconds()) {
				return "", fmt.Errorf("timeout")
			}
			time.Sleep(100 * time.Millisecond)
		}
	}
}

func requestvTPMLaunch(id uuid.UUID, wp *types.WatchdogParam, timeoutSeconds uint) error {
	wk := utils.NewWatchdogKicker(wp.Ps, wp.AgentName, wp.WarnTime, wp.ErrTime)
	body, err := makeRequest(vTPMClient, wk, vtpmLaunchEndpoint, id.String())
	if err != nil {
		return fmt.Errorf("failed to launch vTPM instance: %w (%s)", err, body)
	}

	// Wait for SWTPM to start.
	pidPath := fmt.Sprintf(types.SwtpmPidPath, id.String())
	_, err = utils.GetPidFromFileTimeout(pidPath, timeoutSeconds, wk)
	if err != nil {
		return fmt.Errorf("failed to get pid from file %s: %w", pidPath, err)
	}

	return nil
}

func requestvTPMPurge(id uuid.UUID, wp *types.WatchdogParam) error {
	// Send a request to vTPM control socket, ask it to purge the instance
	// and all its data.
	wk := utils.NewWatchdogKicker(wp.Ps, wp.AgentName, wp.WarnTime, wp.ErrTime)
	body, err := makeRequest(vTPMClient, wk, vtpmPurgeEndpoint, id.String())
	if err != nil {
		return fmt.Errorf("failed to purge vTPM instance: %w (%s)", err, body)
	}

	return nil
}

func requestvTPMTermination(id uuid.UUID, wp *types.WatchdogParam) error {
	// Send a request to vTPM control socket, ask it to terminate the instance.
	wk := utils.NewWatchdogKicker(wp.Ps, wp.AgentName, wp.WarnTime, wp.ErrTime)
	body, err := makeRequest(vTPMClient, wk, vtpmTermEndpoint, id.String())
	if err != nil {
		return fmt.Errorf("failed to terminate vTPM instance: %w (%s)", err, body)
	}

	return nil
}
