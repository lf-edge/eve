// Copyright (c) 2017-2024 Zededa, Inc.
// SPDX-License-Identifier: Apache-2.0

package hypervisor

import (
	"fmt"
	"net"
	"os"
	"path/filepath"
	"runtime"
	"strings"
	"sync/atomic"
	"text/template"
	"time"

	zconfig "github.com/lf-edge/eve-api/go/config"
	"github.com/lf-edge/eve/pkg/pillar/agentlog"
	"github.com/lf-edge/eve/pkg/pillar/containerd"
	"github.com/lf-edge/eve/pkg/pillar/pubsub"
	"github.com/lf-edge/eve/pkg/pillar/types"
	"github.com/lf-edge/eve/pkg/pillar/utils"
	fileutils "github.com/lf-edge/eve/pkg/pillar/utils/file"
	uuid "github.com/satori/go.uuid"
	"github.com/sirupsen/logrus"
	"golang.org/x/sys/unix"
)

const (
	// KVMHypervisorName is a name of kvm hypervisor
	KVMHypervisorName = "kvm"
	minUringKernelTag = uint64((5 << 16) | (4 << 8) | (72 << 0))
	swtpmTimeout      = 10 // seconds
	qemuTimeout       = 3  // seconds
	vtpmPurgePrefix   = "purge;"
	vtpmDeletePrefix  = "terminate;"
	vtpmLaunchPrefix  = "launch;"
)

var clientCid = uint32(unix.VMADDR_CID_HOST + 1)

// We build device model around PCIe topology according to best practices
//    https://github.com/qemu/qemu/blob/master/docs/pcie.txt
// and
//    https://libvirt.org/pci-hotplug.html
// Thus the only PCI devices plugged directly into the root (pci.0) bus are:
//    00:01.0 cirrus-vga
//    00:02.0 pcie-root-port for QEMU XHCI Host Controller
//    00:03.0 virtio-serial for hvc consoles and serial communications with the domain
//    00:0x.0 pcie-root-port for block or network device #x (where x > 2)
//    00:0y.0 virtio-9p-pci
//
// This makes everything but 9P volumes be separated from root pci bus
// and effectively hang off the bus of its own:
//     01:00.0 QEMU XHCI Host Controller (behind pcie-root-port 00:02.0)
//     xx:00.0 block or network device #x (behind pcie-root-port 00:0x.0)
//
// It would be nice to figure out how to do the same with virtio-9p-pci
// eventually, but for now this is not a high priority.
//
// As discussed in https://edk2.groups.io/g/discuss/topic/windows_2019_vm_fails_to_boot/74465994
// I/O size exceeds the max SCSI I/O limitation(8M) of vhost-scsi in KVM
// we adjust max_sectors option (16384) to run Windows VM with vhost-scsi-pci and avoid errors like
// [ 259.573575] vhost_scsi_calc_sgls: requested sgl_count: 2649 exceeds pre-allocated max_sgls: 2048

const qemuConfTemplate = `# This file is automatically generated by domainmgr
[msg]
  timestamp = "on"

[machine]
  type = "{{.Machine}}"
  dump-guest-core = "off"
{{- if .DomainStatus.CPUs }}
  cpumask = "{{.DomainStatus.CPUs}}"
{{- end -}}
{{- if .DomainConfig.CPUsPinned }}
  cpu-pin = "on"
{{- end -}}
{{- if eq .Machine "virt" }}
  accel = "kvm:tcg"
  gic-version = "host"
{{- end -}}
{{- if ne .Machine "virt" }}
  accel = "kvm"
  vmport = "off"
  kernel-irqchip = "on"
{{- end -}}
{{- if .DomainConfig.BootLoader }}
  {{- if or (ne .VirtualizationMode "FML") (eq .Machine "virt") }}
  firmware = "{{.DomainConfig.BootLoader}}"
  {{- end }}
{{- end -}}
{{- if .DomainConfig.Kernel }}
  kernel = "{{.DomainConfig.Kernel}}"
{{- end -}}
{{- if .DomainConfig.Ramdisk }}
  initrd = "{{.DomainConfig.Ramdisk}}"
{{- end -}}
{{- if .DomainConfig.DeviceTree }}
  dtb = "{{.DomainConfig.DeviceTree}}"
{{- end -}}
{{- if .DomainConfig.ExtraArgs }}
  append = "{{.DomainConfig.ExtraArgs}}"
{{- end }}
{{if ne .Machine "virt" }}
[global]
  driver = "kvm-pit"
  property = "lost_tick_policy"
  value = "delay"

[global]
  driver = "ICH9-LPC"
  property = "disable_s3"
  value = "1"

[global]
  driver = "ICH9-LPC"
  property = "disable_s4"
  value = "1"

[rtc]
  base = "localtime"
  driftfix = "slew"

[device]
  driver = "intel-iommu"
  caching-mode = "on"
{{- end }}

{{- if and (eq .VirtualizationMode "FML") (ne .Machine "virt") }}

[drive "drive-ovmf-code"]
  if = "pflash"
  format = "raw"
  readonly = "on"
  unit = "0"
  file = "{{.DomainConfig.BootLoader}}"

[drive "drive-ovmf-vars"]
  if = "pflash"
  format = "raw"
  unit = "1"
  file = "{{.BootLoaderSettingsFile}}"
{{- end }}

[overcommit]
  mem-lock = "off"

[chardev "charmonitor"]
  backend = "socket"
  path = "` + kvmStateDir + `{{.DomainConfig.DisplayName}}/qmp"
  server = "on"
  wait = "off"

[mon "monitor"]
  chardev = "charmonitor"
  mode = "control"

[chardev "charlistener"]
  backend = "socket"
  path = "` + kvmStateDir + `{{.DomainConfig.DisplayName}}/listener.qmp"
  server = "on"
  wait = "off"

[mon "listener"]
  chardev = "charlistener"
  mode = "control"

[memory]
  size = "{{.DomainConfig.Memory}}"

[smp-opts]
  cpus = "{{.DomainConfig.VCpus}}"
  sockets = "1"
  cores = "{{.DomainConfig.VCpus}}"
  threads = "1"

[device]
  driver = "virtio-serial"
  addr = "3"

[chardev "charserial0"]
  backend = "socket"
  mux = "on"
  path = "` + kvmStateDir + `{{.DomainConfig.DisplayName}}/cons"
  server = "on"
  wait = "off"
  logfile = "/dev/fd/1"
  logappend = "on"

[device]
  driver = "virtconsole"
  chardev = "charserial0"
  name = "org.lfedge.eve.console.0"

{{if .DomainConfig.IsOCIContainer}}
[chardev "charserial1"]
  backend = "socket"
  mux = "on"
  path = "` + kvmStateDir + `{{.DomainConfig.DisplayName}}/prime-cons"
  server = "on"
  wait = "off"
  logfile = "/dev/fd/1"
  logappend = "on"

[device]
  driver = "virtconsole"
  chardev = "charserial1"
  name = "org.lfedge.eve.console.prime"

{{- if .DomainConfig.EnableVncShimVM}}
[chardev "charserial2"]
  backend = "vc"

[device]
  driver = "virtconsole"
  chardev = "charserial2"
  name = "org.lfedge.eve.console.prime.forvnc"
{{- end -}}
{{end}}

{{if .DomainConfig.EnableVnc}}
[vnc "default"]
  vnc = "0.0.0.0:{{if .DomainConfig.VncDisplay}}{{.DomainConfig.VncDisplay}}{{else}}0{{end}}"
  to = "99"
{{- if .DomainConfig.VncPasswd}}
  password = "on"
{{- end -}}
{{end}}
#[device "video0"]
#  driver = "qxl-vga"
#  ram_size = "67108864"
#  vram_size = "67108864"
#  vram64_size_mb = "0"
#  vgamem_mb = "16"
#  max_outputs = "1"
#  bus = "pcie.0"
#  addr = "0x1"
{{ if ne .DomainConfig.GPUConfig "" -}}
{{- if ne .Machine "virt" }}
[device "video0"]
  driver = "VGA"
  vgamem_mb = "16"
  bus = "pcie.0"
  addr = "0x1"
{{else}}
[device "video0"]
  driver = "virtio-gpu-pci"
{{end}}
{{- end}}
[device "pci.2"]
  driver = "pcie-root-port"
  port = "12"
  chassis = "2"
  bus = "pcie.0"
  addr = "0x2"

[device "usb"]
  driver = "qemu-xhci"
  p2 = "15"
  p3 = "15"
  bus = "pci.2"
  addr = "0x0"
{{if ne .Machine "virt" }}
[device "input0"]
  driver = "usb-tablet"
  bus = "usb.0"
  port = "1"
{{else}}
[device "input0"]
  driver = "usb-kbd"
  bus = "usb.0"
  port = "1"

[device "input1"]
  driver = "usb-mouse"
  bus = "usb.0"
  port = "2"
{{end}}`

const qemuDiskTemplate = `
{{if eq .Devtype "cdrom"}}
[drive "drive-sata0-{{.DiskID}}"]
  file = "{{.FileLocation}}"
  format = "{{.Format | Fmt}}"
  if = "none"
  media = "cdrom"
  readonly = "on"

[device "sata0-{{.SATAId}}"]
  drive = "drive-sata0-{{.DiskID}}"
{{- if eq .Machine "virt"}}
  driver = "usb-storage"
{{else}}
  driver = "ide-cd"
  bus = "ide.{{.SATAId}}"
{{- end }}
{{else if eq .Devtype "9P"}}
[fsdev "fsdev{{.DiskID}}"]
  fsdriver = "local"
  security_model = "none"
  multidevs = "remap"
  path = "{{.FileLocation}}"

[device "fs{{.DiskID}}"]
  driver = "virtio-9p-pci"
  fsdev = "fsdev{{.DiskID}}"
  mount_tag = "share_dir"
  addr = "{{printf "0x%x" .PCIId}}"
{{else}}
[device "pci.{{.PCIId}}"]
  driver = "pcie-root-port"
  port = "1{{.PCIId}}"
  chassis = "{{.PCIId}}"
  bus = "pcie.0"
  addr = "{{printf "0x%x" .PCIId}}"
{{if eq .WWN ""}}
[drive "drive-virtio-disk{{.DiskID}}"]
  file = "{{.FileLocation}}"
  format = "{{.Format | Fmt}}"
  aio = "{{.AioType}}"
  cache = "writeback"
  if = "none"
{{if .ReadOnly}}  readonly = "on"{{end}}
{{- if eq .Devtype "legacy"}}
[device "ahci.{{.PCIId}}"]
  bus = "pci.{{.PCIId}}"
  driver = "ahci"

[device "ahci-disk{{.DiskID}}"]
  driver = "ide-hd"
  bus = "ahci.{{.PCIId}}.0"
{{- else}}
[device "virtio-disk{{.DiskID}}"]
  driver = "virtio-blk-pci"
  scsi = "off"
  bus = "pci.{{.PCIId}}"
  addr = "0x0"
{{- end}}
  drive = "drive-virtio-disk{{.DiskID}}"
{{- else}}
[device "vhost-disk{{.DiskID}}"]
  driver = "vhost-scsi-pci"
  max_sectors = "16384"
  wwpn = "{{.WWN}}"
  bus = "pci.{{.PCIId}}"
  addr = "0x0"
  num_queues = "{{.NumQueues}}"
{{- end}}
{{end}}`

const qemuNetTemplate = `
[device "pci.{{.PCIId}}"]
  driver = "pcie-root-port"
  port = "1{{.PCIId}}"
  chassis = "{{.PCIId}}"
  bus = "pcie.0"
  multifunction = "on"
  addr = "{{printf "0x%x" .PCIId}}"

[netdev "hostnet{{.NetID}}"]
  type = "tap"
  ifname = "{{.Vif}}"
  br = "{{.Bridge}}"
  script = "/etc/xen/scripts/qemu-ifup"
  downscript = "no"

[device "net{{.NetID}}"]
  driver = "{{.Driver}}"
  netdev = "hostnet{{.NetID}}"
  mac = "{{.Mac}}"
  bus = "pci.{{.PCIId}}"
  addr = "0x0"
{{- if and (eq .Driver "virtio-net-pci") (ne .MTU 0) }}
  host_mtu = "{{.MTU}}"
{{- end}}
`

const qemuPciPassthruTemplate = `
[device "pci.{{.PCIId}}"]
  driver = "pcie-root-port"
  port = "1{{.PCIId}}"
  chassis = "{{.PCIId}}"
  bus = "pcie.0"
  multifunction = "on"
  addr = "{{printf "0x%x" .PCIId}}"

[device]
  driver = "vfio-pci"
  host = "{{.PciShortAddr}}"
  bus = "pci.{{.PCIId}}"
  addr = "0x0"
{{- if .Xvga }}
  x-vga = "on"
{{- end -}}
{{- if .Xopregion }}
  x-igd-opregion = "on"
{{- end -}}
`
const qemuSerialTemplate = `
[chardev "charserial-usr{{.ID}}"]
  backend = "serial"
  path = "{{.SerialPortName}}"

[device "serial-usr{{.ID}}"]
{{- if eq .Machine "virt"}}
  driver = "pci-serial"
{{- else}}
  driver = "isa-serial"
{{- end}}
  chardev = "charserial-usr{{.ID}}"
`

const qemuCANBusTemplate = `
[object "canbus{{.ID}}"]
  qom-type = "can-bus"

[device "{{.IfName}}"]
  driver = "kvaser_pci"
  canbus = "canbus{{.ID}}"

[object "canhost{{.ID}}"]
  qom-type = "can-host-socketcan"
  canbus = "canbus{{.ID}}"
  if = "{{.HostIfName}}"
`

const qemuVsockTemplate = `
[device "eve-vsock0"]
  driver = "vhost-vsock-pci"
  disable-legacy = "on"
  guest-cid = "{{.GuestCID}}"
`

const qemuSwtpmTemplate = `
[chardev "swtpm"]
  backend = "socket"
  path = "{{.CtrlSocket}}"

[tpmdev "tpm0"]
  type = "emulator"
  chardev = "swtpm"

[device "tpm-tis"]
# 'virt' refers to aarch64
# 'tpm-tis-device' for aarch64 versus 'tpm-tis' for x86
# Reference: https://listman.redhat.com/archives/libvir-list/2021-February/msg00647.html
{{- if eq .Machine "virt"}}
  driver = "tpm-tis-device"
{{- else}}
  driver = "tpm-tis"
{{- end}}
  tpmdev = "tpm0"
`

const kvmStateDir = "/run/hypervisor/kvm/"
const sysfsPciDriversProbe = "/sys/bus/pci/drivers_probe"
const vfioDriverPath = "/sys/bus/pci/drivers/vfio-pci"

// KvmContext is a KVM domains map 0-1 to anchor device model UNIX processes (qemu or firecracker)
// For every anchor process we maintain the following entry points in the
// /run/hypervisor/kvm/DOMAIN_NAME:
//
//	 pid - contains PID of the anchor process
//	 qmp - UNIX domain socket that allows us to talk to anchor process
//	cons - symlink to /dev/pts/X that allows us to talk to the serial console of the domain
//
// In addition to that, we also maintain DOMAIN_NAME -> PID mapping in KvmContext, so we don't
// have to look things up in the filesystem all the time (this also allows us to filter domains
// that may be created by others)
type KvmContext struct {
	ctrdContext
	// for now the following is statically configured and can not be changed per domain
	devicemodel  string
	dmExec       string
	dmArgs       []string
	dmCPUArgs    []string
	dmFmlCPUArgs []string
	capabilities *types.Capabilities
}

func newKvm() Hypervisor {
	ctrdCtx, err := initContainerd()
	if err != nil {
		logrus.Fatalf("couldn't initialize containerd (this should not happen): %v. Exiting.", err)
		return nil // it really never returns on account of above
	}
	// later on we may want to pass device model machine type in DomainConfig directly;
	// for now -- lets just pick a static device model based on the host architecture
	// "-cpu host",
	// -cpu IvyBridge-IBRS,ss=on,vmx=on,movbe=on,hypervisor=on,arat=on,tsc_adjust=on,mpx=on,rdseed=on,smap=on,clflushopt=on,sha-ni=on,umip=on,md-clear=on,arch-capabilities=on,xsaveopt=on,xsavec=on,xgetbv1=on,xsaves=on,pdpe1gb=on,3dnowprefetch=on,avx=off,f16c=off,hv_time,hv_relaxed,hv_vapic,hv_spinlocks=0x1fff
	switch runtime.GOARCH {
	case "arm64":
		return KvmContext{
			ctrdContext:  *ctrdCtx,
			devicemodel:  "virt",
			dmExec:       "/usr/lib/xen/bin/qemu-system-aarch64",
			dmArgs:       []string{"-display", "none", "-S", "-no-user-config", "-nodefaults", "-no-shutdown", "-serial", "chardev:charserial0"},
			dmCPUArgs:    []string{"-cpu", "host"},
			dmFmlCPUArgs: []string{"-cpu", "host"},
		}
	case "amd64":
		return KvmContext{
			ctrdContext:  *ctrdCtx,
			devicemodel:  "pc-q35-3.1",
			dmExec:       "/usr/lib/xen/bin/qemu-system-x86_64",
			dmArgs:       []string{"-display", "none", "-S", "-no-user-config", "-nodefaults", "-no-shutdown", "-serial", "chardev:charserial0", "-machine", "hpet=off"},
			dmCPUArgs:    []string{"-cpu", "host"},
			dmFmlCPUArgs: []string{"-cpu", "host,hv_time,hv_relaxed,hv_vendor_id=eveitis,hypervisor=off,kvm=off"},
		}
	}
	return nil
}

// GetCapabilities returns capabilities of the kvm hypervisor
func (ctx KvmContext) GetCapabilities() (*types.Capabilities, error) {
	if ctx.capabilities != nil {
		return ctx.capabilities, nil
	}
	vtd, err := ctx.checkIOVirtualisation()
	if err != nil {
		return nil, fmt.Errorf("fail in check IOVirtualization: %v", err)
	}
	ctx.capabilities = &types.Capabilities{
		HWAssistedVirtualization: true,
		IOVirtualization:         vtd,
		CPUPinning:               true,
		UseVHost:                 true,
	}
	return ctx.capabilities, nil
}

// CountMemOverhead - returns the memory overhead estimation for a domain.
func (ctx KvmContext) CountMemOverhead(domainName string, domainUUID uuid.UUID, domainRAMSize int64, vmmMaxMem int64,
	domainMaxCpus int64, domainVCpus int64, domainIoAdapterList []types.IoAdapter, aa *types.AssignableAdapters,
	globalConfig *types.ConfigItemValueMap) (uint64, error) {
	result, err := vmmOverhead(domainName, domainUUID, domainRAMSize, vmmMaxMem, domainMaxCpus, domainVCpus, domainIoAdapterList, aa, globalConfig)
	return uint64(result), err
}

func (ctx KvmContext) checkIOVirtualisation() (bool, error) {
	f, err := os.Open("/sys/kernel/iommu_groups")
	if err == nil {
		files, err := f.Readdirnames(0)
		if err != nil {
			return false, err
		}
		if len(files) != 0 {
			return true, nil
		}
	}
	return false, err
}

// Name returns the name of the kvm hypervisor
func (ctx KvmContext) Name() string {
	return KVMHypervisorName
}

// Task returns either the kvm context or the containerd context depending on the domain status
func (ctx KvmContext) Task(status *types.DomainStatus) types.Task {
	if status.VirtualizationMode == types.NOHYPER {
		return ctx.ctrdContext
	}
	return ctx
}

func estimatedVMMOverhead(domainName string, aa *types.AssignableAdapters, domainAdapterList []types.IoAdapter,
	domainUUID uuid.UUID, domainRAMSize int64, domainMaxCpus int64, domainVcpus int64) (int64, error) {
	var overhead int64

	mmioOverhead, err := mmioVMMOverhead(domainName, aa, domainAdapterList, domainUUID)

	if err != nil {
		return 0, logError("mmioVMMOverhead() failed for domain %s: %v",
			domainName, err)
	}
	overhead = undefinedVMMOverhead() + ramVMMOverhead(domainRAMSize) +
		qemuVMMOverhead() + cpuVMMOverhead(domainMaxCpus, domainVcpus) + mmioOverhead

	return overhead, nil
}

func ramVMMOverhead(ramMemory int64) int64 {
	// 0.224% of the total RAM allocated for VM in bytes
	// this formula is precise and well explained in the following QEMU issue:
	// https://gitlab.com/qemu-project/qemu/-/issues/1003
	// This is a best case scenario because it assumes that all PTEs are allocated
	// sequentially. In reality, there will be some fragmentation and the overhead
	// for now 2.5% (~10x) is a good approximation until we have a better way to
	// predict the memory usage of the VM.
	return ramMemory * 1024 * 25 / 1000
}

// overhead for qemu binaries and libraries
func qemuVMMOverhead() int64 {
	return 20 << 20 // Mb in bytes
}

// overhead for VMM memory mapped IO
// it fluctuates between 0.66 and 0.81 % of MMIO total size
// for all mapped devices. Set it to 1% to be on the safe side
// this can be a pretty big number for GPUs with very big
// aperture size (e.g. 64G for NVIDIA A40)
func mmioVMMOverhead(domainName string, aa *types.AssignableAdapters, domainAdapterList []types.IoAdapter,
	domainUUID uuid.UUID) (int64, error) {
	var pciAssignments []pciDevice
	var mmioSize uint64

	for _, adapter := range domainAdapterList {
		logrus.Debugf("processing adapter %d %s\n", adapter.Type, adapter.Name)
		aaList := aa.LookupIoBundleAny(adapter.Name)
		// We reserved it in handleCreate so nobody could have stolen it
		if len(aaList) == 0 {
			return 0, logError("IoBundle disappeared %d %s for %s\n",
				adapter.Type, adapter.Name, domainName)
		}
		for _, ib := range aaList {
			if ib == nil {
				continue
			}
			if ib.UsedByUUID != domainUUID {
				return 0, logError("IoBundle not ours %s: %d %s for %s\n",
					ib.UsedByUUID, adapter.Type, adapter.Name,
					domainName)
			}
			if ib.PciLong != "" && ib.UsbAddr == "" {
				logrus.Infof("Adding PCI device <%s>\n", ib.PciLong)
				tap := pciDevice{pciLong: ib.PciLong, ioType: ib.Type}
				pciAssignments = addNoDuplicatePCI(pciAssignments, tap)
			}
		}
	}

	for _, dev := range pciAssignments {
		logrus.Infof("PCI device %s %d\n", dev.pciLong, dev.ioType)
		// read the size of the PCI device aperture. Only GPU/VGA devices for now
		if dev.ioType != types.IoOther && dev.ioType != types.IoHDMI {
			continue
		}
		// skip bridges
		isBridge, err := dev.isBridge()
		if err != nil {
			// do not treat as fatal error
			logrus.Warnf("Can't read PCI device class, treat as bridge %s: %v\n",
				dev.pciLong, err)
			isBridge = true
		}

		if isBridge {
			logrus.Infof("Skipping bridge %s\n", dev.pciLong)
			continue
		}

		// read all resources of the PCI device
		resources, err := dev.readResources()
		if err != nil {
			return 0, logError("Can't read PCI device resources %s: %v\n",
				dev.pciLong, err)
		}

		// calculate the size of the MMIO region
		for _, res := range resources {
			if res.valid() && res.isMem() {
				mmioSize += res.size()
			}
		}
	}

	// 1% of the total MMIO size in bytes
	mmioOverhead := int64(mmioSize) / 100

	logrus.Infof("MMIO size: %d / overhead: %d for %s", mmioSize, mmioOverhead, domainName)

	return int64(mmioOverhead), nil
}

// each vCPU requires about 3MB of memory
func cpuVMMOverhead(maxCpus int64, vcpus int64) int64 {
	cpus := maxCpus
	if cpus == 0 {
		cpus = vcpus
	}
	return cpus * (3 << 20) // Mb in bytes
}

// memory allocated by QEMU for its own purposes.
// statistical analysis did not revile any correlation between
// VM configuration (devices, nr of vcpus, etc) and this number
// however the size of disk space affects it. Probably some internal
// QEMU caches are allocated based on the size of the disk image.
// it requires more investigation.
func undefinedVMMOverhead() int64 {
	return 350 << 20 // Mb in bytes
}

func vmmOverhead(domainName string, domainUUID uuid.UUID, domainRAMSize int64, vmmMaxMem int64, domainMaxCpus int64, domainVCpus int64, domainIoAdapterList []types.IoAdapter, aa *types.AssignableAdapters, globalConfig *types.ConfigItemValueMap) (int64, error) {
	var overhead int64

	// Fetch VMM max memory setting (aka vmm overhead)
	overhead = vmmMaxMem << 10

	// Global node setting has a higher priority
	if globalConfig != nil {
		VmmOverheadOverrideCfgItem, ok := globalConfig.GlobalSettings[types.VmmMemoryLimitInMiB]
		if !ok {
			return 0, logError("Missing key %s", string(types.VmmMemoryLimitInMiB))
		}
		if VmmOverheadOverrideCfgItem.IntValue > 0 {
			overhead = int64(VmmOverheadOverrideCfgItem.IntValue) << 20
		}
	}

	if overhead == 0 {
		overhead, err := estimatedVMMOverhead(domainName, aa, domainIoAdapterList, domainUUID, domainRAMSize, domainMaxCpus, domainVCpus)
		if err != nil {
			return 0, logError("estimatedVMMOverhead() failed for domain %s: %v",
				domainName, err)
		}
		return overhead, nil
	}

	return overhead, nil
}

func getOVMFSettingsFilename(domainName string) (string, error) {
	// Extract the UUID from the domain name. It lets persist the OVMF settings file over domain
	// configuration changes, including domain reactivation (each time the configuration is changed,
	// it updates the domain name, increasing the counter after UUID).
	domainUUID, _, _, err := types.DomainnameToUUID(domainName)
	if err != nil {
		return "", logError("failed to extract UUID from domain name: %v", err)
	}
	return types.OVMFSettingsDir + "/" + domainUUID.String() + "_OVMF_VARS.fd", nil
}

func prepareOVMFSettings(config types.DomainConfig, status types.DomainStatus, globalConfig *types.ConfigItemValueMap) error {
	// Create the OVMF settings directory if it does not exist
	if err := os.MkdirAll(types.OVMFSettingsDir, 0755); err != nil {
		return logError("failed to create OVMF settings directory: %v", err)
	}
	// Create a copy of the ovmf_vars.bin file in <domainName>_ovmf_vars.bin
	ovmfSettingsFile, err := getOVMFSettingsFilename(status.DomainName)
	if err != nil {
		return logError("failed to get OVMF settings file: %v", err)
	}
	// Check if we need custom OVMF settings for the domain (the resolution)
	fmlResolution := types.FmlResolutionUnset
	if config.VirtualizationMode == types.FML {
		// if we are not getting the resolution from the cloud-init, check the
		// global config.
		fmlResolution = status.FmlCustomResolution
		if fmlResolution == types.FmlResolutionUnset {
			if fmlResolution, err = getFmlCustomResolution(&status, globalConfig); err != nil {
				return logError("failed to get custom resolution for domain %s: %v", status.DomainName, err)
			}
		}
	}
	// Find the necessary OVMF settings file
	ovmfSettingsFileSrc := types.OVMFSettingsTemplate
	if fmlResolution != types.FmlResolutionUnset {
		ovmfSettingsFileSrc = types.CustomOVMFSettingsDir + "/OVMF_VARS_" + fmlResolution + ".fd"
	}
	if _, err := os.Stat(ovmfSettingsFile); os.IsNotExist(err) {
		if err := fileutils.CopyFile(ovmfSettingsFileSrc, ovmfSettingsFile); err != nil {
			return logError("failed to copy OVMF_VARS file: %v", err)
		}
	}
	// Set the RW permissions for the OVMF settings file
	if err := os.Chmod(ovmfSettingsFile, 0666); err != nil {
		return logError("failed to set RW permissions for ovmf_vars.bin file: %v", err)
	}
	return nil
}

func cleanupOVMFSettings(domainName string) error {
	ovmfVarsFile, err := getOVMFSettingsFilename(domainName)
	if err != nil {
		return logError("failed to get OVMF settings file: %v", err)
	}
	if err := os.Remove(ovmfVarsFile); err != nil {
		return logError("failed to remove ovmf_vars.bin file: %v", err)
	}
	return nil
}

// Setup sets up kvm
func (ctx KvmContext) Setup(status types.DomainStatus, config types.DomainConfig,
	aa *types.AssignableAdapters, globalConfig *types.ConfigItemValueMap, file *os.File) error {

	diskStatusList := status.DiskStatusList
	domainName := status.DomainName
	domainUUID := status.UUIDandVersion.UUID

	// check if vTPM is enabled
	swtpmCtrlSock := ""
	if status.VirtualTPM {
		domainUUID, _, _, err := types.DomainnameToUUID(domainName)
		if err != nil {
			logError("failed to extract UUID from domain name (vTPM): %v", err)
		} else {
			swtpmCtrlSock = fmt.Sprintf(types.SwtpmCtrlSocketPath, domainUUID)
		}
	}

	// Before we start building the domain config, we need to prepare the OVMF settings.
	// Currently, we only support OVMF settings for FML mode on x86_64 architecture.
	// To support OVMF settings for ARM, we need to add fix OVFM build for ARM to
	// produce separate OVMF_VARS.fd and OVMF_CODE.fd files. Currently, OVMF build
	// for ARM produces a single QEMU_EFI.fd file that contains both OVMF_VARS.fd
	// and OVMF_CODE.fd.
	if config.VirtualizationMode == types.FML && runtime.GOARCH == "amd64" {
		if err := prepareOVMFSettings(config, status, globalConfig); err != nil {
			return logError("failed to setup OVMF settings for domain %s: %v", status.DomainName, err)
		}
	}

	// first lets build the domain config
	if err := ctx.CreateDomConfig(domainName, config, status, diskStatusList,
		aa, globalConfig, swtpmCtrlSock, file); err != nil {
		return logError("failed to build domain config: %v", err)
	}

	dmArgs := ctx.dmArgs
	if config.VirtualizationMode == types.FML {
		dmArgs = append(dmArgs, ctx.dmFmlCPUArgs...)
	} else {
		dmArgs = append(dmArgs, ctx.dmCPUArgs...)
	}

	if config.MetaDataType == types.MetaDataOpenStack {
		// we need to set product_name to support cloud-init
		dmArgs = append(dmArgs, "-smbios", "type=1,product=OpenStack Compute")
	}

	os.MkdirAll(kvmStateDir+domainName, 0777)

	args := []string{ctx.dmExec}
	args = append(args, dmArgs...)
	args = append(args, "-name", domainName,
		"-uuid", domainUUID.String(),
		"-readconfig", file.Name(),
		"-pidfile", kvmStateDir+domainName+"/pid")

	spec, err := ctx.setupSpec(&status, &config, status.OCIConfigDir)

	if err != nil {
		return logError("failed to load OCI spec for domain %s: %v", status.DomainName, err)
	}
	if err = spec.AddLoader(xenToolsPath); err != nil {
		return logError("failed to add kvm hypervisor loader to domain %s: %v", status.DomainName, err)
	}
	overhead, err := vmmOverhead(domainName, domainUUID, int64(config.Memory), int64(config.VMMMaxMem), int64(config.MaxCpus), int64(config.VCpus), config.IoAdapterList, aa, globalConfig)
	if err != nil {
		return logError("vmmOverhead() failed for domain %s: %v",
			status.DomainName, err)
	}
	logrus.Debugf("Qemu overhead for domain %s is %d bytes", status.DomainName, overhead)
	spec.AdjustMemLimit(config, overhead)
	spec.Get().Process.Args = args
	logrus.Infof("Hypervisor args: %v", args)

	spec.GrantFullAccessToDevices()

	if err := spec.CreateContainer(true); err != nil {
		return logError("Failed to create container for task %s from %v: %v", status.DomainName, config, err)
	}

	return nil
}

// Coalesce per-app `EnableVncShimVM` flag and global `debug.enable.vnc.shim.vm`
// debug flag, making sure we don't activate VNC for shim VM if VNC for
// this application is disabled.
func isVncShimVMEnabled(
	globalConfig *types.ConfigItemValueMap, config types.DomainConfig) bool {
	globalShimVnc := false
	if globalConfig != nil {
		item, ok := globalConfig.GlobalSettings[types.VncShimVMAccess]
		globalShimVnc = ok && item.BoolValue
	}
	return config.EnableVnc && (config.EnableVncShimVM || globalShimVnc)
}

func getFmlCustomResolution(status *types.DomainStatus, globalConfig *types.ConfigItemValueMap) (string, error) {
	fmlResolutions := status.FmlCustomResolution
	// if not set in the domain status, try to get it from the global config
	if fmlResolutions == types.FmlResolutionUnset {
		if globalConfig != nil {
			item, ok := globalConfig.GlobalSettings[types.FmlCustomResolution]
			if ok {
				fmlResolutions = item.StringValue()
			}
		}
	}

	// validate the resolution
	switch fmlResolutions {
	case types.FmlResolution800x600,
		types.FmlResolution1024x768,
		types.FmlResolution1280x800,
		types.FmlResolution1920x1080,
		types.FmlResolutionUnset:
		return fmlResolutions, nil
	}

	return "", fmt.Errorf("invalid fml resolution %s", fmlResolutions)
}

// CreateDomConfig creates a domain config (a qemu config file,
// typically named something like xen-%d.cfg)
func (ctx KvmContext) CreateDomConfig(domainName string,
	config types.DomainConfig, status types.DomainStatus,
	diskStatusList []types.DiskStatus, aa *types.AssignableAdapters,
	globalConfig *types.ConfigItemValueMap, swtpmCtrlSock string, file *os.File) error {
	virtualizationMode := ""
	bootLoaderSettingsFile, err := getOVMFSettingsFilename(domainName)
	if err != nil {
		return logError("failed to get OVMF settings file: %v", err)
	}
	if config.VirtualizationMode == types.FML {
		virtualizationMode = "FML"
	}
	tmplCtx := struct {
		Machine                string
		VirtualizationMode     string
		BootLoaderSettingsFile string
		types.DomainConfig
		types.DomainStatus
	}{ctx.devicemodel, virtualizationMode, bootLoaderSettingsFile, config, status}
	tmplCtx.DomainConfig.Memory = (config.Memory + 1023) / 1024
	tmplCtx.DomainConfig.EnableVncShimVM =
		isVncShimVMEnabled(globalConfig, config)
	tmplCtx.DomainConfig.DisplayName = domainName

	// render global device model settings
	t, _ := template.New("qemu").Parse(qemuConfTemplate)
	if err := t.Execute(file, tmplCtx); err != nil {
		return logError("can't write to config file %s (%v)", file.Name(), err)
	}

	// render swtpm settings
	if swtpmCtrlSock != "" {
		swtpmContext := struct {
			Machine    string
			CtrlSocket string
		}{ctx.devicemodel, swtpmCtrlSock}
		t, _ = template.New("qemuSwtpm").Parse(qemuSwtpmTemplate)
		if err := t.Execute(file, swtpmContext); err != nil {
			return logError("can't write to config file %s (%v)", file.Name(), err)
		}
	}

	// render disk device model settings
	diskContext := struct {
		Machine                          string
		PCIId, DiskID, SATAId, NumQueues int
		AioType                          string
		types.DiskStatus
	}{Machine: ctx.devicemodel, PCIId: 4, DiskID: 0, SATAId: 0, AioType: "io_uring", NumQueues: config.VCpus}

	t, _ = template.New("qemuDisk").
		Funcs(template.FuncMap{"Fmt": func(f zconfig.Format) string { return strings.ToLower(f.String()) }}).
		Parse(qemuDiskTemplate)
	for _, ds := range diskStatusList {
		if ds.Devtype == "" {
			continue
		}
		if ds.Devtype == "AppCustom" {
			// This is application custom data. It is forwarded to the VM
			// differently - as a download url in zedrouter
			continue
		}
		diskContext.DiskStatus = ds
		if err := t.Execute(file, diskContext); err != nil {
			return logError("can't write to config file %s (%v)", file.Name(), err)
		}
		if diskContext.Devtype == "cdrom" {
			diskContext.SATAId = diskContext.SATAId + 1
		} else {
			diskContext.PCIId = diskContext.PCIId + 1
		}
		diskContext.DiskID = diskContext.DiskID + 1
	}

	// render network device model settings
	netContext := struct {
		PCIId, NetID     int
		Driver           string
		Mac, Bridge, Vif string
		MTU              uint16
	}{PCIId: diskContext.PCIId, NetID: 0}
	t, _ = template.New("qemuNet").Parse(qemuNetTemplate)
	for _, net := range config.VifList {
		netContext.Mac = net.Mac.String()
		netContext.Bridge = net.Bridge
		netContext.Vif = net.Vif
		if config.VirtualizationMode == types.LEGACY {
			netContext.Driver = "e1000"
		} else {
			netContext.Driver = "virtio-net-pci"
		}
		netContext.MTU = net.MTU
		if err := t.Execute(file, netContext); err != nil {
			return logError("can't write to config file %s (%v)", file.Name(), err)
		}
		netContext.PCIId = netContext.PCIId + 1
		netContext.NetID = netContext.NetID + 1
	}

	// Gather all PCI assignments into a single line
	var pciAssignments []pciDevice
	// Gather all serial assignments into a single line
	var serialAssignments []string
	// Gather all CAN Bus assignments into a single line
	canBusAssignments := make(map[string]string)

	for _, adapter := range config.IoAdapterList {
		logrus.Debugf("processing adapter %d %s\n", adapter.Type, adapter.Name)
		list := aa.LookupIoBundleAny(adapter.Name)
		// We reserved it in handleCreate so nobody could have stolen it
		if len(list) == 0 {
			logrus.Fatalf("IoBundle disappeared %d %s for %s\n",
				adapter.Type, adapter.Name, domainName)
		}
		for _, ib := range list {
			if ib == nil {
				continue
			}
			if ib.UsedByUUID != config.UUIDandVersion.UUID {
				logrus.Fatalf("IoBundle not ours %s: %d %s for %s\n",
					ib.UsedByUUID, adapter.Type, adapter.Name,
					domainName)
			}
			if ib.PciLong != "" && ib.UsbAddr == "" {
				logrus.Infof("Adding PCI device <%v>\n", ib.PciLong)
				tap := pciDevice{pciLong: ib.PciLong, ioType: ib.Type}
				pciAssignments = addNoDuplicatePCI(pciAssignments, tap)
			}
			if ib.Serial != "" {
				logrus.Infof("Adding serial <%s>\n", ib.Serial)
				serialAssignments = addNoDuplicate(serialAssignments, ib.Serial)
			}
			if ib.Type == types.IoLCAN && ib.Ifname != "" {
				var canIfName string
				if ib.Logicallabel == "" {
					canIfName = ib.Phylabel
				} else {
					canIfName = ib.Logicallabel
				}
				if canIfName != "" {
					logrus.Infof("Adding CAN interface <%s>", canIfName)
					if canBusAssignments[canIfName] == "" {
						canBusAssignments[canIfName] = ib.Ifname
					}
				}
			}
		}
	}
	if len(pciAssignments) != 0 {
		pciPTContext := struct {
			PCIId        int
			PciShortAddr string
			Xvga         bool
			Xopregion    bool
		}{PCIId: netContext.PCIId, PciShortAddr: "", Xvga: false, Xopregion: false}

		t, _ = template.New("qemuPciPT").Parse(qemuPciPassthruTemplate)
		for _, pa := range pciAssignments {
			short := types.PCILongToShort(pa.pciLong)
			pciPTContext.Xvga = pa.isVGA()

			if vendor, err := pa.vid(); err == nil {
				// check for Intel vendor
				if vendor == "0x8086" {
					if pciPTContext.Xvga {
						// we set opregion for Intel vga
						// https://github.com/qemu/qemu/blob/stable-5.0/docs/igd-assign.txt#L91-L96
						pciPTContext.Xopregion = true
					}
				}
			}

			pciPTContext.PciShortAddr = short
			if err := t.Execute(file, pciPTContext); err != nil {
				return logError("can't write PCI Passthrough to config file %s (%v)", file.Name(), err)
			}
			pciPTContext.Xvga = false
			pciPTContext.Xopregion = false
			pciPTContext.PCIId = pciPTContext.PCIId + 1
		}
	}
	if len(serialAssignments) != 0 {
		serialPortContext := struct {
			Machine        string
			SerialPortName string
			ID             int
		}{Machine: ctx.devicemodel, SerialPortName: "", ID: 0}

		t, _ = template.New("qemuSerial").Parse(qemuSerialTemplate)
		for id, serial := range serialAssignments {
			serialPortContext.SerialPortName = serial
			fmt.Printf("id for serial is %d\n", id)
			serialPortContext.ID = id
			if err := t.Execute(file, serialPortContext); err != nil {
				return logError("can't write serial assignment to config file %s (%v)", file.Name(), err)
			}
		}
	}
	if len(canBusAssignments) != 0 {
		canIfContext := struct {
			Machine    string
			IfName     string
			HostIfName string
			ID         int
		}{Machine: ctx.devicemodel, IfName: "", HostIfName: "", ID: 0}

		t, err := template.New("qemuCANBus").Parse(qemuCANBusTemplate)
		if err != nil {
			return logError("can't create CAN Bus configuration template: %v", err)
		}
		id := 0
		for canIf, canHostIf := range canBusAssignments {
			logrus.Infof("CAN interface %s connected to host CAN %s\n", canIf, canHostIf)
			canIfContext.IfName = canIf
			canIfContext.HostIfName = canHostIf
			canIfContext.ID = id
			id++
			if err := t.Execute(file, canIfContext); err != nil {
				return logError("can't write CAN Bus assignment to config file %s (%v)", file.Name(), err)
			}
		}
	}

	// render vsock settings, this should go last to avoid
	// PCI ID conflicts, let qemu assign PCI ID for vsock.
	vsockContext := struct {
		GuestCID string
	}{
		// currently we don't save the clientCid/AppUUID pair since
		// we there is no need for it, but in the future when wen
		// we add channels for vms to report things like CPU/Mem usage
		// then it makes sense to keep track of who is who.
		GuestCID: fmt.Sprintf("%d",
			// clientCid needs atomic add to avoid race condition
			// in case CreateDomConfig is called concurrently, which
			// happens at least in unit tests.
			atomic.AddUint32(&clientCid, 1))}
	t, _ = template.New("qemuVsock").Parse(qemuVsockTemplate)
	if err := t.Execute(file, vsockContext); err != nil {
		return logError("can't write to config file %s (%v)", file.Name(), err)
	}

	return nil
}

// waitForQmp does the waiting/retry in getQemuStatus, and ignores its returned
// status.
func waitForQmp(domainName string, available bool) error {
	var err error
	sock := GetQmpExecutorSocket(domainName)
	logrus.Infof("waitForQmp for %s %t",
		domainName, available)
	if _, err = getQemuStatus(sock); available == (err == nil) {
		logrus.Infof("waitForQmp for %s %t done", domainName, available)
		return nil
	}
	if available {
		return logError("Giving up waiting to connect to QEMU Monitor Protocol socket %s from VM %s, error: %v",
			sock, domainName, err)
	}
	return logError("Giving up waiting to cleanup VM %s, QEMU Monitor Protocol socket %s is still available",
		domainName, sock)

}

// Start starts a domain
func (ctx KvmContext) Start(domainName string) error {
	logrus.Infof("starting KVM domain %s", domainName)
	if err := ctx.ctrdContext.Start(domainName); err != nil {
		logrus.Errorf("couldn't start task for domain %s: %v", domainName, err)
		return err
	}
	logrus.Infof("done launching qemu device model")
	if err := waitForQmp(domainName, true); err != nil {
		logrus.Errorf("Error waiting for Qmp for domain %s: %v", domainName, err)
		return err
	}
	logrus.Infof("done launching qemu device model")

	qmpFile := GetQmpExecutorSocket(domainName)

	logrus.Debugf("starting qmpEventHandler")
	logrus.Infof("Creating %s at %s", "qmpEventHandler", agentlog.GetMyStack())
	go qmpEventHandler(getQmpListenerSocket(domainName), GetQmpExecutorSocket(domainName))

	annotations, err := ctx.ctrdContext.Annotations(domainName)
	if err != nil {
		logrus.Warnf("Error in get annotations for domain %s: %v", domainName, err)
		return err
	}

	if vncPassword, ok := annotations[containerd.EVEOCIVNCPasswordLabel]; ok && vncPassword != "" {
		if err := execVNCPassword(qmpFile, vncPassword); err != nil {
			return logError("failed to set VNC password %v", err)
		}
	}

	if err := execContinue(qmpFile); err != nil {
		return logError("failed to start domain that is stopped %v", err)
	}

	if status, err := getQemuStatus(qmpFile); err != nil || status != types.RUNNING {
		return logError("domain status is not running but %s after cont command returned %v", status, err)
	}
	return nil
}

// Stop stops a domain
func (ctx KvmContext) Stop(domainName string, _ bool) error {
	if err := execShutdown(GetQmpExecutorSocket(domainName)); err != nil {
		return logError("Stop: failed to execute shutdown command %v", err)
	}
	return nil
}

// Delete deletes a domain
func (ctx KvmContext) Delete(domainName string) (result error) {
	//Sending a stop signal to then domain before quitting. This is done to freeze the domain before quitting it.
	_, err := os.Stat(GetQmpExecutorSocket(domainName))
	if err == nil {
		execStop(GetQmpExecutorSocket(domainName))
		if err = execQuit(GetQmpExecutorSocket(domainName)); err != nil {
			return logError("failed to execute quit command %v", err)
		}
	}
	// we may want to wait a little bit here and actually kill qemu process if it gets wedged
	if err := os.RemoveAll(kvmStateDir + domainName); err != nil {
		return logError("failed to clean up domain state directory %s (%v)", domainName, err)
	}

	return nil
}

// Info returns information of a domain
func (ctx KvmContext) Info(domainName string) (int, types.SwState, error) {
	// first we ask for the task status
	effectiveDomainID, effectiveDomainState, err := ctx.ctrdContext.Info(domainName)
	if err != nil || effectiveDomainState != types.RUNNING {
		return effectiveDomainID, effectiveDomainState, err
	}

	_, err = getQemuStatus(GetQmpExecutorSocket(domainName))
	if err != nil {
		return effectiveDomainID, types.BROKEN,
			logError("couldn't retrieve status for domain %s: %v", domainName, err)
	}

	return effectiveDomainID, effectiveDomainState, nil
}

// Cleanup cleans up a domain
func (ctx KvmContext) Cleanup(domainName string) error {
	if err := ctx.ctrdContext.Cleanup(domainName); err != nil {
		return fmt.Errorf("couldn't cleanup task %s: %v", domainName, err)
	}
	if err := waitForQmp(domainName, false); err != nil {
		return fmt.Errorf("error waiting for Qmp absent for domain %s: %v", domainName, err)
	}

	// Cleanup OVMF settings
	settingsFile, err := getOVMFSettingsFilename(domainName)
	if err != nil {
		return fmt.Errorf("failed to get OVMF settings file: %v", err)
	}
	if _, err := os.Stat(settingsFile); err == nil {
		if err := cleanupOVMFSettings(domainName); err != nil {
			return fmt.Errorf("failed to cleanup OVMF settings for domain %s: %v", domainName, err)
		}
	}
	return nil
}

// PCIReserve reserves a PCI device
func (ctx KvmContext) PCIReserve(long string) error {
	return PCIReserveGeneric(long)
}

// PCIRelease releases the PCI device reservation
func (ctx KvmContext) PCIRelease(long string) error {
	return PCIReleaseGeneric(long)
}

// PCISameController checks if two PCI controllers are the same
func (ctx KvmContext) PCISameController(id1 string, id2 string) bool {
	return PCISameControllerGeneric(id1, id2)
}

func usbBusPort(USBAddr string) (string, string) {
	ids := strings.SplitN(USBAddr, ":", 2)
	if len(ids) == 2 {
		return ids[0], ids[1]
	}
	return "", ""
}

// GetQmpExecutorSocket returns the path to the qmp socket of a domain
func GetQmpExecutorSocket(domainName string) string {
	return filepath.Join(kvmStateDir, domainName, "qmp")
}

func getQmpListenerSocket(domainName string) string {
	return filepath.Join(kvmStateDir, domainName, "listener.qmp")
}

// VirtualTPMSetup launches a vTPM instance for the domain
func (ctx KvmContext) VirtualTPMSetup(domainName, agentName string, ps *pubsub.PubSub, warnTime, errTime time.Duration) error {
	if ps != nil {
		wk := utils.NewWatchdogKick(ps, agentName, warnTime, errTime)
		domainUUID, _, _, err := types.DomainnameToUUID(domainName)
		if err != nil {
			return fmt.Errorf("failed to extract UUID from domain name (vTPM setup): %v", err)
		}
		return requestVtpmLaunch(domainUUID, wk, swtpmTimeout)
	}

	return fmt.Errorf("invalid watchdog configuration (vTPM setup)")
}

// VirtualTPMTerminate terminates the vTPM instance
func (ctx KvmContext) VirtualTPMTerminate(domainName string) error {
	domainUUID, _, _, err := types.DomainnameToUUID(domainName)
	if err != nil {
		return fmt.Errorf("failed to extract UUID from domain name (vTPM terminate): %v", err)
	}
	if err := requestVtpmTermination(domainUUID); err != nil {
		return fmt.Errorf("failed to terminate vTPM for domain %s: %w", domainName, err)
	}
	return nil
}

// VirtualTPMTeardown purges the vTPM instance.
func (ctx KvmContext) VirtualTPMTeardown(domainName string) error {
	domainUUID, _, _, err := types.DomainnameToUUID(domainName)
	if err != nil {
		return fmt.Errorf("failed to extract UUID from domain name (vTPM teardown): %v", err)
	}
	if err := requestVtpmPurge(domainUUID); err != nil {
		return fmt.Errorf("failed to purge vTPM for domain %s: %w", domainName, err)
	}

	return nil
}

func requestVtpmLaunch(id uuid.UUID, wk *utils.WatchdogKick, timeoutSeconds uint) error {
	conn, err := net.Dial("unix", types.VtpmdCtrlSocket)
	if err != nil {
		return fmt.Errorf("failed to connect to vTPM control socket: %w", err)
	}
	defer conn.Close()

	pidPath := fmt.Sprintf(types.SwtpmPidPath, id.String())

	// Send the request to the vTPM control socket, ask it to launch a swtpm instance.
	_, err = conn.Write([]byte(fmt.Sprintf("%s%s\n", vtpmLaunchPrefix, id.String())))
	if err != nil {
		return fmt.Errorf("failed to write to vTPM control socket: %w", err)
	}

	// Loop and wait for SWTPM to start.
	pid, err := utils.GetPidFromFileTimeout(pidPath, timeoutSeconds, wk)
	if err != nil {
		return fmt.Errorf("failed to get pid from file %s: %w", pidPath, err)
	}

	// One last time, check SWTPM is not dead right after launch.
	if !utils.IsProcAlive(pid) {
		return fmt.Errorf("SWTPM (pid: %d) is dead", pid)
	}

	return nil
}

func requestVtpmPurge(id uuid.UUID) error {
	conn, err := net.Dial("unix", types.VtpmdCtrlSocket)
	if err != nil {
		return fmt.Errorf("failed to connect to vTPM control socket: %w", err)
	}
	defer conn.Close()

	// Send a request to vTPM control socket, ask it to purge the instance
	// and all its data.
	_, err = conn.Write([]byte(fmt.Sprintf("%s%s\n", vtpmPurgePrefix, id.String())))
	if err != nil {
		return fmt.Errorf("failed to write to vTPM control socket: %w", err)
	}

	return nil
}

func requestVtpmTermination(id uuid.UUID) error {
	conn, err := net.Dial("unix", types.VtpmdCtrlSocket)
	if err != nil {
		return fmt.Errorf("failed to connect to vTPM control socket: %w", err)
	}
	defer conn.Close()

	// Send a request to the vTPM control socket, ask it to delete the instance.
	_, err = conn.Write([]byte(fmt.Sprintf("%s%s\n", vtpmDeletePrefix, id.String())))
	if err != nil {
		return fmt.Errorf("failed to write to vTPM control socket: %w", err)
	}

	return nil
}
